{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "297a6371064289d7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "computational-james",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-transmission",
   "metadata": {
    "executionInfo": {
     "elapsed": 2360,
     "status": "ok",
     "timestamp": 1650012512493,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "wJWGvzLaj3Y7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta, datetime\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import pickle\n",
    "import sys\n",
    "import subprocess\n",
    "from glob import glob\n",
    "import csv\n",
    "import math\n",
    "import codecs\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# EDA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP & ML \n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    fbeta_score,\n",
    "    roc_auc_score,\n",
    "    matthews_corrcoef,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "\n",
    "# DL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    InputSpec,\n",
    "    Layer, \n",
    "    Input,\n",
    "    Embedding, \n",
    "    Conv1D, \n",
    "    Conv2D, \n",
    "    Bidirectional, \n",
    "    Dense, \n",
    "    Attention,\n",
    "    LSTM, \n",
    "    Activation, \n",
    "    Add, \n",
    "    Flatten, \n",
    "    Concatenate, \n",
    "    concatenate,\n",
    "    Reshape, \n",
    "    Dropout, \n",
    "    SpatialDropout1D, \n",
    "    BatchNormalization,\n",
    "    MaxPooling1D, \n",
    "    MaxPool2D, \n",
    "    GlobalAveragePooling1D, \n",
    "    GlobalMaxPooling1D, \n",
    "    GlobalMaxPool1D\n",
    ")\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-jacksonville",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ploting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-stopping",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_values(pc, fmt=\"%.2f\", **kw):\n",
    "    '''\n",
    "    Heatmap with text in each cell with matplotlib's pyplot\n",
    "    Source: https://stackoverflow.com/a/25074150/395857 \n",
    "    By HYRY\n",
    "    '''\n",
    "#     from itertools import izip\n",
    "    pc.update_scalarmappable()\n",
    "    ax = pc.axes# FOR LATEST MATPLOTLIB\n",
    "    \n",
    "    #Use zip BELOW IN PYTHON 3\n",
    "    for p, color, value in zip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.all(color[:3] > 0.5):\n",
    "            color = (0.0, 0.0, 0.0)\n",
    "        else:\n",
    "            color = (1.0, 1.0, 1.0)\n",
    "        # ax.text(x, y, fmt % value, ha=\"center\", va=\"center\", color=color, **kw)\n",
    "\n",
    "\n",
    "def cm2inch(*tupl):\n",
    "    '''\n",
    "    Specify figure size in centimeter in matplotlib\n",
    "    Source: https://stackoverflow.com/a/22787457/395857\n",
    "    By gns-ank\n",
    "    '''\n",
    "    inch = 2.54\n",
    "    if type(tupl[0]) == tuple:\n",
    "        return tuple(i/inch for i in tupl[0])\n",
    "    else:\n",
    "        return tuple(i/inch for i in tupl)\n",
    "\n",
    "\n",
    "def heatmap(AUC, title, xlabel, ylabel, xticklabels, yticklabels, figure_width=40, figure_height=20, correct_orientation=False, cmap='RdBu'):\n",
    "    '''\n",
    "    Inspired by:\n",
    "    - https://stackoverflow.com/a/16124677/395857 \n",
    "    - https://stackoverflow.com/a/25074150/395857\n",
    "    '''\n",
    "\n",
    "    # Plot it out\n",
    "    fig, ax = plt.subplots()    \n",
    "    #c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap='RdBu', vmin=0.0, vmax=1.0)\n",
    "    c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap=cmap)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)\n",
    "\n",
    "    # set tick labels\n",
    "    #ax.set_xticklabels(np.arange(1,AUC.shape[1]+1), minor=False)\n",
    "    ax.set_xticklabels(xticklabels, minor=False)\n",
    "    ax.set_yticklabels(yticklabels, minor=False)\n",
    "\n",
    "    # set title and x/y labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)      \n",
    "    \n",
    "    # save the figure\n",
    "    plt.savefig('reports/' + title + '_' + datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "    plt.savefig('reports/' + title + '_' + datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    # Remove last blank column\n",
    "    plt.xlim( (0, AUC.shape[1]) )\n",
    "\n",
    "    # Turn off all the ticks\n",
    "    ax = plt.gca()    \n",
    "    for t in ax.xaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "    for t in ax.yaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "\n",
    "    # Add color bar\n",
    "    plt.colorbar(c)\n",
    "\n",
    "    # Add text in each cell \n",
    "    show_values(c)\n",
    "\n",
    "    # Proper orientation (origin at the top left instead of bottom left)\n",
    "    if correct_orientation:\n",
    "        ax.invert_yaxis()\n",
    "        ax.xaxis.tick_top()       \n",
    "\n",
    "    # resize \n",
    "    fig = plt.gcf()\n",
    "    #fig.set_size_inches(cm2inch(40, 20))\n",
    "    #fig.set_size_inches(cm2inch(40*4, 20*4))\n",
    "    fig.set_size_inches(cm2inch(figure_width, figure_height))\n",
    "    \n",
    "\n",
    "#\n",
    "def plot_classification_report(classification_report, title='Classification report ', cmap='RdBu'):\n",
    "    '''\n",
    "    Plot scikit-learn classification report.\n",
    "    Extension based on https://stackoverflow.com/a/31689645/395857 \n",
    "    '''\n",
    "    lines = classification_report.split('\\n')\n",
    "\n",
    "    classes = []\n",
    "    plotMat = []\n",
    "    support = []\n",
    "    class_names = []\n",
    "    for line in lines[2 : (len(lines) - 4)]:\n",
    "        t = line.strip().split()\n",
    "        if len(t) < 2: continue\n",
    "        classes.append(t[0])\n",
    "        v = [float(x) for x in t[1: len(t) - 1]]\n",
    "        support.append(int(t[-1]))\n",
    "        class_names.append(t[0])\n",
    "        print(v)\n",
    "        plotMat.append(v)\n",
    "\n",
    "    print('plotMat: {0}'.format(plotMat))\n",
    "    print('support: {0}'.format(support))\n",
    "\n",
    "    xlabel = 'Metrics'\n",
    "    ylabel = 'Classes'\n",
    "    xticklabels = ['Precision', 'Recall', 'F1-score']\n",
    "    yticklabels = ['{0} ({1})'.format(class_names[idx], sup) for idx, sup  in enumerate(support)]\n",
    "    figure_width = 25\n",
    "    figure_height = len(class_names) + 7\n",
    "    correct_orientation = False\n",
    "    heatmap(np.array(plotMat), title, xlabel, ylabel, xticklabels, yticklabels, figure_width, figure_height, correct_orientation, cmap=cmap)\n",
    "    \n",
    "# Function to plot the training and validation loss and accuracy\n",
    "def plot_loss_accuracy(history, model_name):\n",
    "    # plot the training and validation loss\n",
    "    plt.plot(history.epoch, history.history['loss'], '-o', label='Training_loss')\n",
    "    plt.plot(history.epoch, history.history['val_loss'], '-o', label='Validation_loss')\n",
    "    plt.title(model_name + ' model loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xlim(left=0)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig('reports/' + model_name + '_loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "    plt.savefig('reports/' + model_name + '_loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # plot the training and validation accuracy\n",
    "    plt.plot(history.epoch, history.history['accuracy'], '-o', label='Training_accuracy')\n",
    "    plt.plot(history.epoch, history.history['val_accuracy'], '-o', label='Validation_accuracy')\n",
    "    plt.title(model_name + ' model accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xlim(left=0)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.savefig('reports/' + model_name + '_accuracy' + datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "    plt.savefig('reports/' + model_name + '_accuracy' + datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_class_distribution(data, title):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    # sns.set(style=\"ticks\")\n",
    "    ax = sns.countplot(x='label', data=data)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Annotate the bars with the number of samples\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='baseline', fontsize=11, color='black', xytext=(0, 5),\n",
    "                    textcoords='offset points')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datasets importation and EDA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d216d44615ad1468"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import the train, validation and the test sets\n",
    "print('Loading datasets...')\n",
    "train_set= pd.read_csv('datasets/training_set.csv')\n",
    "val_set = pd.read_csv('datasets/validation_set.csv')\n",
    "test_set = pd.read_csv('datasets/test_set.csv')\n",
    "\n",
    "# import all augmented dataset\n",
    "train_set_ch = pd.read_csv('datasets/vishing_dataset_CH_AUG.csv')\n",
    "train_set_en = pd.read_csv('datasets/vishing_dataset_EN_AUG.csv')\n",
    "train_set_ja = pd.read_csv('datasets/vishing_dataset_JA_AUG.csv')\n",
    "\n",
    "print('Datasets loaded.')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e857ebb099d48eb6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set.info()\n",
    "print('-'*80)\n",
    "val_set.info()\n",
    "print('-'*80)\n",
    "test_set.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9f5dd456c31dda1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set_en.info()\n",
    "print('-'*80)\n",
    "train_set_ja.info()\n",
    "print('-'*80)\n",
    "train_set_ch.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0f7d34b9cac3555",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Delete all the rows in train_set_en, train_set_ja and train_set_ch that have label 0\n",
    "train_set_ch = train_set_ch[train_set_ch['label'] != 0]\n",
    "train_set_en = train_set_en[train_set_en['label'] != 0]\n",
    "train_set_ja = train_set_ja[train_set_ja['label'] != 0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "728addbe7c6a49a2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot the distribution of the classes in the datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb753cf31e42fb5e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the distribution of the datasets\n",
    "plot_class_distribution(train_set, 'Train Dataset Class Distribution')\n",
    "plot_class_distribution(val_set, 'Validation Dataset Class Distribution')\n",
    "plot_class_distribution(test_set, 'Test Dataset Class Distribution')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "701ec442c0befe16",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the distribution of the datasets\n",
    "plot_class_distribution(train_set_en, '(English Augmented) Train Dataset Class Distribution')\n",
    "plot_class_distribution(train_set_ja, '(Japanese Augmented) Train Dataset Class Distribution')\n",
    "plot_class_distribution(train_set_ch, '(Chinese Augmented) Train Dataset Class Distribution')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1e2acaa99c288b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# drop the colum we don't need\n",
    "train_set.drop(['id'], axis=1, inplace=True)\n",
    "val_set.drop(['id'], axis=1, inplace=True)\n",
    "test_set.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "#check the dataframes\n",
    "train_set.info()\n",
    "print('-'*80)\n",
    "val_set.info()\n",
    "print('-'*80)\n",
    "test_set.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c083b7a03240e15c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# drop the colum we don't need\n",
    "train_set_en.drop(['id', 'transcript', 'translation', 'processed'], axis=1, inplace=True)\n",
    "train_set_ja.drop(['id', 'transcript', 'translation', 'processed'], axis=1, inplace=True)\n",
    "train_set_ch.drop(['id', 'transcript', 'translation', 'processed'], axis=1, inplace=True)\n",
    "\n",
    "#check the dataframes\n",
    "train_set_en.info()\n",
    "print('-'*80)\n",
    "train_set_ja.info()\n",
    "print('-'*80)\n",
    "train_set_ch.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fd262ad4d49817a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# rename the column back_translation of train_set_en, train_set_ja, train_set_ch to transcript_en, transcript_ja, transcript_ch\n",
    "train_set_en.rename(columns={'back_translation':'transcript_en'}, inplace=True)\n",
    "train_set_ja.rename(columns={'back_translation':'transcript_ja'}, inplace=True)\n",
    "train_set_ch.rename(columns={'back_translation':'transcript_ch'}, inplace=True)\n",
    "\n",
    "# display the info of the dataframes\n",
    "train_set_en.info()\n",
    "print('-'*80)\n",
    "train_set_ja.info()\n",
    "print('-'*80)\n",
    "train_set_ch.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b628d2ba1a34650",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Calculating the length of each data sample."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56112131c97d54e9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# calculate the length of each data sample in the train_set, val_set and test_set and add the length as a new column named length to the dataframes\n",
    "train_set['length'] = train_set['transcript'].apply(lambda x: len(x))\n",
    "val_set['length'] = val_set['transcript'].apply(lambda x: len(x))\n",
    "test_set['length'] = test_set['transcript'].apply(lambda x: len(x))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e8990a1dea85eff",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# display the heads of the train_set and sort the dataframe by length\n",
    "train_set.head().sort_values(by=['length'], ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3639d56c3e0b7ef3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "val_set.head().sort_values(by=['length'], ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e050899678e28626",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_set.head().sort_values(by=['length'], ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69e796b0f94a60ef",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# calculate the length of each data samples in the train_set_en, train_set_ja and train_set_ch and add the length as a new column named length to the dataframes\n",
    "train_set_en['length'] = train_set_en['transcript_en'].apply(lambda x: len(x))\n",
    "train_set_ja['length'] = train_set_ja['transcript_ja'].apply(lambda x: len(x))\n",
    "train_set_ch['length'] = train_set_ch['transcript_ch'].apply(lambda x: len(x))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "603847d2572d4fd9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# display the heads of the train_set_en and sort the dataframe by length\n",
    "train_set_en.head().sort_values(by=['length'], ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27036334045f9fe9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set_ja.head().sort_values(by=['length'], ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94bb3ce82b4d263b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set_ch.head().sort_values(by=['length'], ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bbd3f4a0b1d3b57",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distribution based on length of words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43bd4a74b402eb56"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Make a function to plot the distribution of the length of the data samples in the train_set, val_set and test_set (boxplot and histogram)\n",
    "def plot_length_distribution(data, title):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    # sns.set(style=\"ticks\")\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    # plot the boxplot\n",
    "    # sns.boxplot(x='length', data=data, ax=ax1)\n",
    "    sns.boxplot(y='length', x='label', data=data, ax=ax1)\n",
    "    ax1.set_title('Boxplot')\n",
    "    \n",
    "    # plot the histogram\n",
    "    sns.histplot(x='length', data=data, ax=ax2)\n",
    "    ax2.set_title('Histogram')\n",
    "    \n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "beae16e14bbbc34b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plot the distribution of the length of the data samples in the train_set, val_set and test_set\n",
    "plot_length_distribution(train_set, 'Train Dataset Length Distribution')\n",
    "plot_length_distribution(val_set, 'Validation Dataset Length Distribution')\n",
    "plot_length_distribution(test_set, 'Test Dataset Length Distribution')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdd222569626a037",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plot the distribution of the length of the data samples in the train_set_en, train_set_ja and train_set_ch\n",
    "plot_length_distribution(train_set_en, '(English Augmented) Train Dataset Length Distribution')\n",
    "plot_length_distribution(train_set_ja, '(Japanese Augmented) Train Dataset Length Distribution')\n",
    "plot_length_distribution(train_set_ch, '(Chinese Augmented) Train Dataset Length Distribution')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf787b25ef6f9354",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Concatenate the augmented datasets to the original train_set dataframe\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3fc2887cad1d112"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Make new train_set dataset with only train_set['transcript'] and train_set['label'] columns.\n",
    "train_set_new = train_set[['transcript', 'label']]\n",
    "train_set_new.columns = ['transcript', 'label']\n",
    "train_set_new"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8256ab7febe25c69",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Make new train_set_en dataset with only train_set_en['transcript_en'] and train_set_en['label'] columns. Rename the columns to 'transcript_en' to 'transcript'.\n",
    "train_set_en_new = train_set_en[['transcript_en', 'label']]\n",
    "train_set_en_new.columns = ['transcript', 'label']\n",
    "train_set_en_new"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dd81feec3e69b81",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# concatenate the augmented train_set_en dataset with the original dataset to create the new train_set dataset for training the models with back-translation method as text augmentation method.\n",
    "train_set_EnKo = pd.concat([train_set_new, train_set_en_new], ignore_index=True)\n",
    "train_set_EnKo = train_set_EnKo.sample(frac=1).reset_index(drop=True)\n",
    "train_set_EnKo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb5b682885dc542",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plot the class distribution of the new train_set dataset \n",
    "plot_class_distribution(train_set_EnKo, 'En-Ko Augmented Train Set Class Distribution')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6b37c35d2177aff",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Make new train_set_ch dataset with only train_set_ch['transcript_ch'] and train_set_ch['label'] columns. Rename the columns to 'transcript_ch' to 'transcript'.\n",
    "train_set_ch_new = train_set_ch[['transcript_ch', 'label']]\n",
    "train_set_ch_new.columns = ['transcript', 'label']\n",
    "train_set_ch_new"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1a79dfa39195966",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# concatenate the augmented train_set_ch dataset with the original dataset to create the new train_set dataset for training the models with back-translation method as text augmentation method.\n",
    "train_set_ChKo = pd.concat([train_set_new, train_set_ch_new], ignore_index=True)\n",
    "train_set_ChKo = train_set_ChKo.sample(frac=1).reset_index(drop=True)\n",
    "train_set_ChKo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3afca11e1119918b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plot the class distribution of the new train_set dataset\n",
    "plot_class_distribution(train_set_ChKo, 'Ch-Ko Augmented Train Set Class Distribution')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8020e9d591d1b65",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Make new train_set_ja dataset with only train_set_ja['transcript_ja'] and train_set_ja['label'] columns. Rename the columns to 'transcript_ja' to 'transcript'.\n",
    "train_set_ja_new = train_set_ja[['transcript_ja', 'label']]\n",
    "train_set_ja_new.columns = ['transcript', 'label']\n",
    "train_set_ja_new"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a04cc99f81197f8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# concatenate the augmented train_set_ja dataset with the original dataset to create the new train_set dataset for training the models with back-translation method as text augmentation method.\n",
    "train_set_JaKo = pd.concat([train_set_new, train_set_ja_new], ignore_index=True)\n",
    "train_set_JaKo = train_set_JaKo.sample(frac=1).reset_index(drop=True)\n",
    "train_set_JaKo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e94ba9197948cc26",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plot the class distribution of the new train_set dataset\n",
    "plot_class_distribution(train_set_JaKo, 'Ja-Ko Augmented Train Set Class Distribution')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49acad3327ce5105",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# concatenate the augmented train_set_new, train_set_en_new, train_set_ch_new, train_set_ja_new dataset to a new dataset\n",
    "train_set_all = pd.concat([train_set_new, train_set_en_new, train_set_ch_new, train_set_ja_new], ignore_index=True)\n",
    "train_set_all = train_set_all.sample(frac=1).reset_index(drop=True)\n",
    "train_set_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2d7356416359506",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plot the class distribution of the new train_set dataset\n",
    "plot_class_distribution(train_set_all, 'All BT Augmented Train Set Class Distribution')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77ca1adf03de15f9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save all the new train_set dataset to csv files\n",
    "train_set_EnKo.to_csv('datasets/train_set_EnKo.csv', index=False)\n",
    "train_set_ChKo.to_csv('datasets/train_set_ChKo.csv', index=False)\n",
    "train_set_JaKo.to_csv('datasets/train_set_JaKo.csv', index=False)\n",
    "train_set_all.to_csv('datasets/train_set_all.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41a08594e4e4f05e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef87eee10b384c97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import all the train, validation and test sets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91a1d731a8c9eb5b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import the train, validation and the test sets\n",
    "print('Loading datasets...')\n",
    "train_set= pd.read_csv('datasets/training_set.csv')\n",
    "val_set = pd.read_csv('datasets/validation_set.csv')\n",
    "test_set = pd.read_csv('datasets/test_set.csv')\n",
    "\n",
    "# import all augmented dataset\n",
    "train_set_EnKo = pd.read_csv('datasets/train_set_EnKo.csv')\n",
    "train_set_ChKo = pd.read_csv('datasets/train_set_ChKo.csv')\n",
    "train_set_JaKo = pd.read_csv('datasets/train_set_JaKo.csv')\n",
    "train_set_all = pd.read_csv('datasets/train_set_all.csv')\n",
    "\n",
    "print('Datasets loaded.')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58636b54ad594858",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# display the heads of the train_set, val_set and test_set\n",
    "train_set.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bad31aace1cc318",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# in one figure, plot all the class distribution of all the augmented train_set dataset and original train_set dataset.\n",
    "sns.set(style=\"whitegrid\")\n",
    "# sns.set(style=\"ticks\")\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(25, 5))\n",
    "fig.suptitle('All Train Set Class Distribution')\n",
    "\n",
    "# plot the class distribution of the original train_set dataset\n",
    "sns.countplot(x='label', data=train_set, ax=ax1)\n",
    "ax1.set_title('Original Train Set')\n",
    "\n",
    "# plot the class distribution of the En-Ko augmented train_set dataset\n",
    "sns.countplot(x='label', data=train_set_EnKo, ax=ax2)\n",
    "ax2.set_title('En-Ko Augmented Train Set')\n",
    "\n",
    "# plot the class distribution of the Ch-Ko augmented train_set dataset\n",
    "sns.countplot(x='label', data=train_set_ChKo, ax=ax3)\n",
    "ax3.set_title('Ch-Ko Augmented Train Set')\n",
    "\n",
    "# plot the class distribution of the Ja-Ko augmented train_set dataset\n",
    "sns.countplot(x='label', data=train_set_JaKo, ax=ax4)\n",
    "ax4.set_title('Ja-Ko Augmented Train Set')\n",
    "\n",
    "# plot the class distribution of the All augmented train_set dataset\n",
    "sns.countplot(x='label', data=train_set_all, ax=ax5)\n",
    "ax5.set_title('All Augmented Train Set')\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ee3ec5106e71265",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installation of mecab-ko and mecab-ko-dic for Korean text preprocessing, Morphology Analyzer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1de807edb6a2053"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3e9e0f8980311ae",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cd Mecab-ko-for-Google-Colab"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bb5dac8573bb0a4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!bash install_mecab-ko_on_colab_light_220429.sh"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecd0a39a91abe054",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Check if Mecab-ko-for-GoogleColab is installed. If not, install it.\n",
    "# try:\n",
    "#     from konlpy.tag import Mecab\n",
    "# except:\n",
    "#     print('Mecab-ko-for-GoogleColab is not installed. Installing...')\n",
    "#     # subprocess.check_call(['bash', './install_mecab-ko_on_colab190912.sh'])\n",
    "#     # print('Mecab-ko-for-GoogleColab installed.')\n",
    "#     print('run the following command in the terminal: bash ./install_mecab-ko_on_colab190912.sh')\n",
    "# \n",
    "# \n",
    "# # if not os.path.exists('/content/Mecab-ko-for-Google-Colab'):\n",
    "# #     print('Installing Mecab-ko-for-Google-Colab...')\n",
    "# #     !git clone"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64d524337d83c533",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing the Mecab-ko"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b47e0ca58538fd45"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test the Mecab-ko\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "add0b40403d51095",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text = '네이버에 중고나라 사이트 아시죠? 대구지역 있으세요 아시죠? 사이트에서 김재원 일단...'\n",
    "print(mecab.morphs(text))\n",
    "print(mecab.pos(text))\n",
    "print(mecab.nouns(text))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e870fc3c0c75dd89",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset cleaning and purification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2c88493ccbeb7fd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# functions to perform the cleaning parts\n",
    "def apply_replacement(src_df, replace_func):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original one\n",
    "    ret_df = src_df.copy()\n",
    "    \n",
    "    # Apply the replacement function to each element in the 'transcript_cleaned' column\n",
    "    ret_df.loc[:, 'transcript_cleaned'] = ret_df['transcript_cleaned'].apply(replace_func)\n",
    "    # ret_df['transcript_cleaned'] = ret_df['transcript_cleaned'].apply(lambda x: replace_func(x))\n",
    "    \n",
    "    return ret_df\n",
    "\n",
    "# remove the unwanted word and characters from the dataset\n",
    "def word_replace(x):\n",
    "    example_word_replace_list = {'o/': '',\n",
    "                                 'b/': '',\n",
    "                                 'n/': '',\n",
    "                                 '\\n': ' ',\n",
    "                                 'name': '',\n",
    "                                 'laughing': '',\n",
    "                                 'clearing': '',\n",
    "                                 'singing': '',\n",
    "                                 'applauding': ''}\n",
    "    for i in example_word_replace_list:\n",
    "        x = x.replace(i, example_word_replace_list[i])\n",
    "    return x\n",
    "\n",
    "# remove the special character from the transcripts\n",
    "# def remove_special_symbols(sentence):\n",
    "#     sentence = re.sub(r\"[^a-zA-Z0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]\", '', sentence)\n",
    "#     return sentence\n",
    "\n",
    "def remove_special_symbols(sentence): \n",
    "    sentence = re.sub(r\"[-~=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]\", '', sentence)\n",
    "    return sentence\n",
    "\n",
    "# Function to delete placeholder words like X, x, O, o, ㅇ, 0, O, o that are repeated more than 2 times in a row\n",
    "def replace_x_o(sentence):\n",
    "    sentence = re.sub(r\"([xXoOㅇ0]{2,})\", '', sentence)\n",
    "    return sentence\n",
    "\n",
    "# remove the unwanted word and characters from the transcripts\n",
    "def nline_replace(x):\n",
    "    example_word_replace_list = {'\\n' : ' '}\n",
    "    for i in example_word_replace_list:\n",
    "        x = x.replace(i, example_word_replace_list[i])\n",
    "    return x\n",
    "\n",
    "# Function to delete extra white space between words\n",
    "def remove_extra_white_spaces(sentence):\n",
    "    sentence = re.sub(r\"\\s+\", ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "# def remove_extra_white_spaces(text):\n",
    "#     single_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
    "#     without_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
    "#     return without_sc\n",
    "\n",
    "# Function to delete all the numbers and digits from the transcripts\n",
    "def remove_numbers(sentence):\n",
    "    sentence = re.sub(r\"\\d+\", '', sentence)\n",
    "    return sentence\n",
    "\n",
    "# A function to perform the cleaning parts on the dataset after duplicating the column transcript to transcript_cleaned and put it next to the original column transcript\n",
    "def clean_dataset(dataset):\n",
    "    # duplicate the original transcript to keep track of the processing\n",
    "    dataset['transcript_cleaned'] = dataset['transcript']\n",
    "    \n",
    "    # move the column transcript_cleaned next to the original column transcript\n",
    "    dataset = dataset[['transcript', 'transcript_cleaned', 'label']]\n",
    "    \n",
    "    # apply the cleaning functions to the dataset\n",
    "    dataset = apply_replacement(dataset, word_replace)\n",
    "    dataset = apply_replacement(dataset, remove_special_symbols)\n",
    "    dataset = apply_replacement(dataset, replace_x_o)\n",
    "    dataset = apply_replacement(dataset, nline_replace)\n",
    "    dataset = apply_replacement(dataset, remove_extra_white_spaces)\n",
    "    dataset = apply_replacement(dataset, remove_numbers)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "291cb3ae98181b97",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# clean the train_set, val_set and test_set and save them in a new column named transcript_cleaned next to the original column transcript in the dataframes.\n",
    "train_set = clean_dataset(train_set)\n",
    "val_set = clean_dataset(val_set)\n",
    "test_set = clean_dataset(test_set)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b30cb9ee06cc7e0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# display the heads of the train_set, val_set and test_set\n",
    "train_set.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae06d22f35019d56",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "val_set.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45e0e46a8cf007b1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_set.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fed31d309e78ad3a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# clean the train_set_EnKo, train_set_ChKo, train_set_JaKo and train_set_all\n",
    "train_set_EnKo = clean_dataset(train_set_EnKo)\n",
    "train_set_ChKo = clean_dataset(train_set_ChKo)\n",
    "train_set_JaKo = clean_dataset(train_set_JaKo)\n",
    "train_set_all = clean_dataset(train_set_all)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f86eabbb26609d34",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# display the heads of the train_set_EnKo, train_set_ChKo, train_set_JaKo and train_set_all\n",
    "train_set_EnKo.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b521e9335b1390f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set_ChKo.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62089461bdeb0f7e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set_JaKo.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a811cd6b70f4bb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set_all.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8274e365a02aec2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # add a column named length to the datasets and calculate the length of each transcript_cleaned and add it to the length column\n",
    "# train_set['length'] = train_set['transcript_cleaned'].apply(lambda x: len(x))\n",
    "# val_set['length'] = val_set['transcript_cleaned'].apply(lambda x: len(x))\n",
    "# test_set['length'] = test_set['transcript_cleaned'].apply(lambda x: len(x))\n",
    "# \n",
    "# train_set_EnKo['length'] = train_set_EnKo['transcript_cleaned'].apply(lambda x: len(x))\n",
    "# train_set_ChKo['length'] = train_set_ChKo['transcript_cleaned'].apply(lambda x: len(x))\n",
    "# train_set_JaKo['length'] = train_set_JaKo['transcript_cleaned'].apply(lambda x: len(x))\n",
    "# train_set_all['length'] = train_set_all['transcript_cleaned'].apply(lambda x: len(x))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "728aa54bcc84aa42",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove the stopwords from the transcripts\n",
    "we will use the stopwords-ko-simple.txt file to remove the stopwords from the transcripts.It contains few stopwords that are commonly used in Korean language.\n",
    "We will further use the stopwords-ko.txt file to remove the stopwords from the transcripts. It contains more stopwords than the stopwords-ko-simple.txt file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef12874d22fe3590"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import the stopwords from the file stopwords-ko-simple.txt and create a list of stopwords\n",
    "def import_stopwords(file_name):    \n",
    "    stopwords = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            stopwords.append(line.strip())\n",
    "\n",
    "    print(stopwords)\n",
    "    \n",
    "    return stopwords"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5de9b9f9552d84fe",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# using the function, import the stopwords from the file stopwords-ko-simple.txt\n",
    "stopwords = import_stopwords('stopwords-ko-simple.txt')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cfa91cf29eece0a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # korean stopwords list\n",
    "# stopwords = [\"을\", \"를\", \"이\", \"가\", \"ㅡ\", \"은\", \"는\", \"XXX\", \"xxx\", \"어요\", \"아니\", \"입니다\", \"에서\", \"에서\", \"니까\", \"으로\", \"근데\", \"습니다\", \"습니까\", \"저희\", \"합니다\", \"하고\", \"싶어요\", \"있는\", \"있습니다\", \"싶습니다\", \"그냥\", \"고요\", \"에요\", \"예요\", \"으시\", \"그래서\"]\n",
    "\n",
    "# Function to remove the stop word from the train and test dataframe\n",
    "def get_model_input(_words):\n",
    "    global stopwords\n",
    "    _words = [x for x in _words if x[0] not in stopwords]\n",
    "    _words = [x for x in _words if x[:-1] not in stopwords]\n",
    "\n",
    "    for i in range(len(_words)-1):\n",
    "        yield _words[i]\n",
    "        \n",
    "def get_corpus(df):\n",
    "    corpus = []\n",
    "    for lwords in df:    \n",
    "        temp = []\n",
    "        for x in get_model_input(lwords):\n",
    "            if len(x) != 1:\n",
    "                temp.append(\"{}\".format(x))\n",
    "        corpus.append(\" \".join(temp))\n",
    "    return corpus "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3241b3374f8eb32",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# remove the stopwords from the train_set, val_set, test_set, train_set_EnKo, train_set_ChKo, train_set_JaKo and train_set_all after tokenization with Mecab-ko\n",
    "# train_set['transcript_cleaned00'] = train_set['transcript_cleaned'].apply(lambda x: ' '.join([word for word in mecab.morphs(x) if word not in (stopwords)]))\n",
    "# val_set['transcript_cleaned'] = val_set['transcript_cleaned'].apply(lambda x: ' '.join([word for word in mecab.morphs(x) if word not in (stopwords)]))\n",
    "# test_set['transcript_cleaned'] = test_set['transcript_cleaned'].apply(lambda x: ' '.join([word for word in mecab.morphs(x) if word not in (stopwords)]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bd5fdfa642c699f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# train_set_EnKo['transcript_cleaned'] = train_set_EnKo['transcript_cleaned'].apply(lambda x: ' '.join([word for word in mecab.morphs(x) if word not in (stopwords)]))\n",
    "# train_set_ChKo['transcript_cleaned'] = train_set_ChKo['transcript_cleaned'].apply(lambda x: ' '.join([word for word in mecab.morphs(x) if word not in (stopwords)]))\n",
    "# train_set_JaKo['transcript_cleaned'] = train_set_JaKo['transcript_cleaned'].apply(lambda x: ' '.join([word for word in mecab.morphs(x) if word not in (stopwords)]))\n",
    "# train_set_all['transcript_cleaned'] = train_set_all['transcript_cleaned'].apply(lambda x: ' '.join([word for word in mecab.morphs(x) if word not in (stopwords)]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28532540deb30e1d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "train_set_corpus = get_corpus([(mecab.morphs(x)) for x in train_set['transcript_cleaned']])\n",
    "val_set_corpus = get_corpus([(mecab.morphs(x)) for x in val_set['transcript_cleaned']])\n",
    "test_set_corpus = get_corpus([(mecab.morphs(x)) for x in test_set['transcript_cleaned']])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a176993ddb6d57e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set_EnKo_corpus = get_corpus([(mecab.morphs(x)) for x in train_set_EnKo['transcript_cleaned']])\n",
    "train_set_ChKo_corpus = get_corpus([(mecab.morphs(x)) for x in train_set_ChKo['transcript_cleaned']])\n",
    "train_set_JaKo_corpus = get_corpus([(mecab.morphs(x)) for x in train_set_JaKo['transcript_cleaned']])\n",
    "train_set_all_corpus = get_corpus([(mecab.morphs(x)) for x in train_set_all['transcript_cleaned']])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46d0262d174803c5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# adding the corpus to the dataframe\n",
    "train_set['corpus'] = train_set_corpus\n",
    "val_set['corpus'] = val_set_corpus\n",
    "test_set['corpus'] = test_set_corpus\n",
    "\n",
    "train_set_EnKo['corpus'] = train_set_EnKo_corpus\n",
    "train_set_ChKo['corpus'] = train_set_ChKo_corpus\n",
    "train_set_JaKo['corpus'] = train_set_JaKo_corpus\n",
    "train_set_all['corpus'] = train_set_all_corpus"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "307bb2a306045fd9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# add the length of the transcripts, transcript_cleaned and corpus to the dataframes\n",
    "train_set['length'] = train_set['transcript'].apply(lambda x: len(x))\n",
    "val_set['length'] = val_set['transcript'].apply(lambda x: len(x))\n",
    "test_set['length'] = test_set['transcript'].apply(lambda x: len(x))\n",
    "\n",
    "train_set_EnKo['length'] = train_set_EnKo['transcript'].apply(lambda x: len(x))\n",
    "train_set_ChKo['length'] = train_set_ChKo['transcript'].apply(lambda x: len(x))\n",
    "train_set_JaKo['length'] = train_set_JaKo['transcript'].apply(lambda x: len(x))\n",
    "train_set_all['length'] = train_set_all['transcript'].apply(lambda x: len(x))\n",
    "\n",
    "train_set['length_cleaned'] = train_set['transcript_cleaned'].apply(lambda x: len(x))\n",
    "val_set['length_cleaned'] = val_set['transcript_cleaned'].apply(lambda x: len(x))\n",
    "test_set['length_cleaned'] = test_set['transcript_cleaned'].apply(lambda x: len(x))\n",
    "\n",
    "train_set_EnKo['length_cleaned'] = train_set_EnKo['transcript_cleaned'].apply(lambda x: len(x))\n",
    "train_set_ChKo['length_cleaned'] = train_set_ChKo['transcript_cleaned'].apply(lambda x: len(x))\n",
    "train_set_JaKo['length_cleaned'] = train_set_JaKo['transcript_cleaned'].apply(lambda x: len(x))\n",
    "train_set_all['length_cleaned'] = train_set_all['transcript_cleaned'].apply(lambda x: len(x))\n",
    "\n",
    "train_set['length_corpus'] = train_set['corpus'].apply(lambda x: len(x))\n",
    "val_set['length_corpus'] = val_set['corpus'].apply(lambda x: len(x))\n",
    "test_set['length_corpus'] = test_set['corpus'].apply(lambda x: len(x))\n",
    "\n",
    "train_set_EnKo['length_corpus'] = train_set_EnKo['corpus'].apply(lambda x: len(x))\n",
    "train_set_ChKo['length_corpus'] = train_set_ChKo['corpus'].apply(lambda x: len(x))\n",
    "train_set_JaKo['length_corpus'] = train_set_JaKo['corpus'].apply(lambda x: len(x))\n",
    "train_set_all['length_corpus'] = train_set_all['corpus'].apply(lambda x: len(x))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f863793c46858299",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "238bb2636e73e15d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9a66597174302b2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set_all.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dea76cb4bb3a6803",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# using the function, import the stopwords from the file stopwords-ko-simple.txt\n",
    "stopwords = import_stopwords('stopwords-ko.txt')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7de62711f84fde00",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set_corpus_1 = get_corpus([(mecab.morphs(x)) for x in train_set['transcript_cleaned']])\n",
    "val_set_corpus_1 = get_corpus([(mecab.morphs(x)) for x in val_set['transcript_cleaned']])\n",
    "test_set_corpus_1 = get_corpus([(mecab.morphs(x)) for x in test_set['transcript_cleaned']])\n",
    "\n",
    "train_set_EnKo_corpus_1 = get_corpus([(mecab.morphs(x)) for x in train_set_EnKo['transcript_cleaned']])\n",
    "train_set_ChKo_corpus_1 = get_corpus([(mecab.morphs(x)) for x in train_set_ChKo['transcript_cleaned']])\n",
    "train_set_JaKo_corpus_1 = get_corpus([(mecab.morphs(x)) for x in train_set_JaKo['transcript_cleaned']])\n",
    "train_set_all_corpus_1 = get_corpus([(mecab.morphs(x)) for x in train_set_all['transcript_cleaned']])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51e101dd95e04fac",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# adding the corpus to the dataframe\n",
    "train_set['corpus_1'] = train_set_corpus_1\n",
    "val_set['corpus_1'] = val_set_corpus_1\n",
    "test_set['corpus_1'] = test_set_corpus_1\n",
    "\n",
    "train_set_EnKo['corpus_1'] = train_set_EnKo_corpus_1\n",
    "train_set_ChKo['corpus_1'] = train_set_ChKo_corpus_1\n",
    "train_set_JaKo['corpus_1'] = train_set_JaKo_corpus_1\n",
    "train_set_all['corpus_1'] = train_set_all_corpus_1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "631d44dfd799f7cb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# adding the length of corpus_1 to the dataframe \n",
    "train_set['length_corpus_1'] = train_set['corpus_1'].apply(lambda x: len(x))\n",
    "val_set['length_corpus_1'] = val_set['corpus_1'].apply(lambda x: len(x))\n",
    "test_set['length_corpus_1'] = test_set['corpus_1'].apply(lambda x: len(x))\n",
    "\n",
    "train_set_EnKo['length_corpus_1'] = train_set_EnKo['corpus_1'].apply(lambda x: len(x))\n",
    "train_set_ChKo['length_corpus_1'] = train_set_ChKo['corpus_1'].apply(lambda x: len(x))\n",
    "train_set_JaKo['length_corpus_1'] = train_set_JaKo['corpus_1'].apply(lambda x: len(x))\n",
    "train_set_all['length_corpus_1'] = train_set_all['corpus_1'].apply(lambda x: len(x))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "986f398a2357f280",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82d9d2b09b63a048",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# order the columns in the dataframes to have the label column after the corpus column\n",
    "train_set = train_set[['transcript', 'transcript_cleaned', 'corpus', 'corpus_1', 'label', 'length', 'length_cleaned', 'length_corpus', 'length_corpus_1']]\n",
    "val_set = val_set[['transcript', 'transcript_cleaned', 'corpus', 'corpus_1', 'label', 'length', 'length_cleaned', 'length_corpus', 'length_corpus_1']]\n",
    "test_set = test_set[['transcript', 'transcript_cleaned', 'corpus', 'corpus_1', 'label', 'length', 'length_cleaned', 'length_corpus', 'length_corpus_1']]\n",
    "\n",
    "train_set_EnKo = train_set_EnKo[['transcript', 'transcript_cleaned', 'corpus', 'corpus_1', 'label', 'length', 'length_cleaned', 'length_corpus', 'length_corpus_1']]\n",
    "train_set_ChKo = train_set_ChKo[['transcript', 'transcript_cleaned', 'corpus', 'corpus_1', 'label', 'length', 'length_cleaned', 'length_corpus', 'length_corpus_1']]\n",
    "train_set_JaKo = train_set_JaKo[['transcript', 'transcript_cleaned', 'corpus', 'corpus_1', 'label', 'length', 'length_cleaned', 'length_corpus', 'length_corpus_1']]\n",
    "train_set_all = train_set_all[['transcript', 'transcript_cleaned', 'corpus', 'corpus_1', 'label', 'length', 'length_cleaned', 'length_corpus', 'length_corpus_1']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79251a1d6d95ccb2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99cd3b0046c20881",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set_all.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "281086d4b0408e03",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the datasets to csv files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea881243a512ca84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save all the dataframes to csv files\n",
    "train_set.to_csv('datasets/train_set_ready.csv', index=False)\n",
    "val_set.to_csv('datasets/val_set_ready.csv', index=False)\n",
    "test_set.to_csv('datasets/test_set_ready.csv', index=False)\n",
    "\n",
    "train_set_EnKo.to_csv('datasets/train_set_EnKo_ready.csv', index=False)\n",
    "train_set_ChKo.to_csv('datasets/train_set_ChKo_ready.csv', index=False)\n",
    "train_set_JaKo.to_csv('datasets/train_set_JaKo_ready.csv', index=False)\n",
    "train_set_all.to_csv('datasets/train_set_all_ready.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3947e8e1352fefc3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vectorization of the datasets\n",
    "Encoding the text data into numerical data for the deep learning models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b0e29107b025942"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#load the datasets\n",
    "print('Loading datasets...')\n",
    "train_set= pd.read_csv('datasets/train_set_ready.csv')\n",
    "val_set = pd.read_csv('datasets/val_set_ready.csv')\n",
    "test_set = pd.read_csv('datasets/test_set_ready.csv')\n",
    "\n",
    "# import all augmented dataset\n",
    "train_set_EnKo = pd.read_csv('datasets/train_set_EnKo_ready.csv')\n",
    "train_set_ChKo = pd.read_csv('datasets/train_set_ChKo_ready.csv')\n",
    "train_set_JaKo = pd.read_csv('datasets/train_set_JaKo_ready.csv')\n",
    "train_set_all = pd.read_csv('datasets/train_set_all_ready.csv')\n",
    "\n",
    "print('Datasets loaded.')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4b353981ed38768",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# define the train and test sets data from the dataframes\n",
    "X_train = train_set['corpus']\n",
    "X_train_1 = train_set['corpus_1']\n",
    "y_train = train_set['label']\n",
    "\n",
    "X_val = val_set['corpus']\n",
    "X_val_1 = val_set['corpus_1']\n",
    "y_val = val_set['label']\n",
    "\n",
    "X_test = test_set['corpus']\n",
    "X_test_1 = test_set['corpus_1']\n",
    "y_test = test_set['label']\n",
    "\n",
    "X_train_EnKo = train_set_EnKo['corpus']\n",
    "X_train_EnKo_1 = train_set_EnKo['corpus_1']\n",
    "y_train_EnKo = train_set_EnKo['label']\n",
    "\n",
    "X_train_ChKo = train_set_ChKo['corpus']\n",
    "X_train_ChKo_1 = train_set_ChKo['corpus_1']\n",
    "y_train_ChKo = train_set_ChKo['label']\n",
    "\n",
    "X_train_JaKo = train_set_JaKo['corpus']\n",
    "X_train_JaKo_1 = train_set_JaKo['corpus_1']\n",
    "y_train_JaKo = train_set_JaKo['label']\n",
    "\n",
    "X_train_all = train_set_all['corpus']\n",
    "X_train_all_1 = train_set_all['corpus_1']\n",
    "y_train_all = train_set_all['label']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21e6e14d157da4fe",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to define the maximum number of words to be used\n",
    "def max_words_func(X_train, X_val, X_test):\n",
    "    max_len = 0\n",
    "    max_words = 0\n",
    "    \n",
    "    # check the length of the trainset, the valset and the testset\n",
    "    print('Train set size = {} \\nValidation set size = {} \\nTest set size = {}'.format(len(X_train), len(X_val), len(X_test)))\n",
    "    \n",
    "    # count the maximum number of words in the trainset\n",
    "    max_words = len(set(\" \".join(X_train).split())) # max number of words for tokenizer\n",
    "    print('Maximum number of words in the train set = {}'.format(max_words))\n",
    "    \n",
    "    # get the maximum length of the sentences in the trainset\n",
    "    max_len = max([len(x.split()) for x in X_train]) # max length of each sentences, including padding\n",
    "    print('Maximum length of a sentence in the train set = {}'.format(max_len))\n",
    "\n",
    "    return max_words, max_len"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c295ae7fb961a7d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# define the maximum number of words and the maximum length of the sentences in the trainset\n",
    "max_words, max_len = max_words_func(X_train, X_val, X_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72f2f23593ed4252",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.hist([len(s) for s in X_train], bins=20)\n",
    "plt.xlabel('Length of Data')\n",
    "plt.ylabel('Number of Data')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a30a4d36047d5035",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.hist([len(s) for s in X_test], bins=20)\n",
    "plt.xlabel('Length of Data')\n",
    "plt.ylabel('Number of Data')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e33ddd44583ca683",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Traininig with the original train_set dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f416c15395795df"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to tokenize the text data\n",
    "def tokenize_text(X_train, X_val, X_test, max_words=max_words, max_len=max_len):\n",
    "    # define the tokenizer and tokenization\n",
    "    tokenizer = Tokenizer(num_words=max_words, lower=False, char_level=False)\n",
    "    tokenizer.fit_on_texts(X_train)  #leaky\n",
    "    \n",
    "    # Function to convert texts to sequences of integers\n",
    "    def texts_to_sequences(X_train, X_val, X_test):\n",
    "        # convert texts to sequences of integers\n",
    "        X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "        X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "        X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "        word_index = tokenizer.word_index\n",
    "        \n",
    "        print(\"Dictionary size: \", len(word_index))\n",
    "        \n",
    "        return X_train_seq, X_val_seq, X_test_seq, word_index\n",
    "    \n",
    "    # define the sequences of integers\n",
    "    X_train_seq, X_val_seq, X_test_seq, word_index = texts_to_sequences(X_train, X_val, X_test)\n",
    "    \n",
    "    # FPad the sequences to ensure uniform length\n",
    "    X_train_pad = pad_sequences(X_train_seq, padding='pre', maxlen=max_len)\n",
    "    X_val_pad = pad_sequences(X_val_seq, padding='pre', maxlen=max_len)\n",
    "    X_test_pad = pad_sequences(X_test_seq, padding='pre', maxlen=max_len)\n",
    "    \n",
    "    return X_train_pad, X_val_pad, X_test_pad, word_index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89e659a18a85f818",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tokenize the text data\n",
    "X_train_pad, X_val_pad, X_test_pad, word_index = tokenize_text(X_train, X_val, X_test, max_words, max_len)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35b10bf5ddd0fc1a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# encoding the data to numerical data\n",
    "# define the tokenizer and tokenization\n",
    "# tokenizer = Tokenizer(num_words=max_words, lower=False, char_level=False)\n",
    "# tokenizer.fit_on_texts(X_train)  #leaky"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7009e50d3d3d8ba1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Function to convert texts to sequences of integers\n",
    "# def texts_to_sequences(X_train, X_val, X_test):\n",
    "#     # convert texts to sequences of integers\n",
    "#     X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "#     X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "#     X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "#     word_index = tokenizer.word_index\n",
    "#     \n",
    "#     print(\"Dictionary size: \", len(word_index))\n",
    "#     \n",
    "#     return X_train_seq, X_val_seq, X_test_seq, word_index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e736cc1f0071fc3c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# define the sequences of integers\n",
    "# X_train_seq, X_val_seq, X_test_seq, word_index = texts_to_sequences(X_train, X_val, X_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b539e39341ead5d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Building and Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "356f63c9492ba87e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing the FastText word embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b70dd5195612f20"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# run if the wiki.ko.vec is not available in the same directory\n",
    "import os\n",
    "import urllib.request\n",
    "# check if the file wiki.ko.vec is in the directory if not download it\n",
    "if not os.path.isfile('wiki.ko.vec'):\n",
    "    print('wiki.ko.vec does not exist, downloading file from the internet')\n",
    "    # download the FastText word embeddings and monitor the download progress bar with tqdm\n",
    "    with tqdm(unit='B', unit_scale=True, miniters=1, desc='wiki.ko.vec') as t:\n",
    "        urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.ko.vec', 'wiki.ko.vec', reporthook=lambda a,b,c: t.update(c))\n",
    "    # urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.ko.vec', 'wiki.ko.vec')\n",
    "else:\n",
    "    print('wiki.ko.vec exists, will not download file from the internet')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "357614d51612fad7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import the FastText word embeddings\n",
    "print('Loading word FastText embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('wiki.ko.vec', encoding='utf-8')\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29a103be30767f56",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('Found %s word vectors' % len(embeddings_index))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d79507de0f1a4c06",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # create a weight matrix for words in training docs\n",
    "# embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "# for word, i in word_index.items():\n",
    "#     if i < max_words:\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             embedding_matrix[i] = embedding_vector"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac071c36786d65f8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # define the embedding layer\n",
    "# embedding_layer = Embedding(max_words,\n",
    "#                             embedding_dim,\n",
    "#                             embeddings_initializer=Constant(embedding_matrix),\n",
    "#                             input_length=max_len,\n",
    "#                             trainable=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "879c2f9b4d072c75",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# define the preprocessing parameters\n",
    "embedding_dim = 300 # embedding dimensions for word vectors (word2vec/GloVe/Fasttext)\n",
    "\n",
    "# defining the hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size= 32 # 64, 128\n",
    "epochs = 10\n",
    "steps_per_epoch = len(X_train) // batch_size  # total_samples is the training set size\n",
    "\n",
    "# Calculating decay steps\n",
    "# It's common to decay the learning rate at each epoch\n",
    "decay_steps = steps_per_epoch * epochs # 10000\n",
    "decay_rate = 0.9  # This is a common decay rate, but you can adjust it\n",
    "# learning_decay = 1e-10 # 1e-4\n",
    "\n",
    "spa_dropout_ratio = 0.2 # dropout ration, dropping a entire feature map\n",
    "kernel_size = 3 # [1,2,3,5] # [1,2,3,5] Size of the kernel. Mixing kernels of various sizes.\n",
    "                # specifying the length of the 1D convolution window.\n",
    "dense_units = 64 # hidden unit 128 the number of neurons in the hidden layer\n",
    "dropout_ratio = 0.2 # 0.1, 0.2 to 0.5 Dropout Ratio\n",
    "num_filters = 50 # 36, 128, 256 number of kernels, conv_size\n",
    "                # Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "\n",
    "lstm_units_1 = 64 # the size(dim) of the hidden state vector as well as the output vector.\n",
    "lstm_units_2 = 32 # the size(dim) of the hidden state vector as well as the output vector."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5951bee5a0d0fcf3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing FastText embedding matrix for future use"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99bded3c2c7b7c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#embedding matrix\n",
    "# print('Preparing embedding matrix for future use...')\n",
    "words_not_found = []\n",
    "embed_dim = embedding_dim #300 # 32 Dimensions of the embedding vector\n",
    "nb_words = min(max_words, len(word_index))\n",
    "embedding_matrix = np.zeros((max_words, embed_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f18cf14ef79ccfc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Setting up our results dataframe\n",
    "df_results = pd.DataFrame(columns=['F1_score', 'Precision', 'Recall', 'Accuracy', 'Training time'])\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3150406220c6a16",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training with default embedding layer and the original train_set dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12dabfd8c5ab6c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single LSTM layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e76aec593e9c1704"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# write a function to build, train and evaluate the LSTM model\n",
    "def build_train_lstm_model(model_name, X_train, y_train, X_val, y_val, X_test, y_test, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, embedding_matrix=None):\n",
    "    \n",
    "    # define the model architecture\n",
    "    model = Sequential()\n",
    "    # check if the embedding_matrix is None or not\n",
    "    if embedding_matrix is None:\n",
    "        model.add(Embedding(max_words, embedding_dim, input_length=max_len, trainable=False))\n",
    "    else:\n",
    "        model.add(Embedding(max_words, embedding_dim, weights= [embedding_matrix], input_length=max_len, trainable=False))\n",
    "    # model.add(SpatialDropout1D(spa_dropout_ratio))\n",
    "    model.add(LSTM(lstm_units_1))\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    # model.add(LSTM(lstm_units_1, dropout=dropout_ratio, recurrent_dropout=dropout_ratio))\n",
    "    model.add(Dropout(dropout_ratio))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "    \n",
    "    # compile the model\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        # optimizer=Adam(learning_rate=learning_rate, decay=learning_rate / decay_steps),\n",
    "        optimizer = Adam(learning_rate=lr_schedule),\n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # print the model summary\n",
    "    print(model.summary())\n",
    "    print('#' * 150)\n",
    "    \n",
    "    # Create a callback that saves the best model during validation\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        \"models/\" + model_name + \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".h5\",\n",
    "        # \"models/LSTM_\"+ datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".h5\",\n",
    "        monitor='val_accuracy', # val_loss or val_accuracy \n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='max' # max for val_accuracy, min for val_loss\n",
    "    )\n",
    "    # EarlyStopping to stop training when the validation loss has stopped improving\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        verbose = 1,\n",
    "        patience = 3, # Number of epochs with no improvement after which training will be stopped.\n",
    "        mode = 'min'\n",
    "    )\n",
    "\n",
    "    # TensorBoard for visualization\n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # train the model\n",
    "    start_time = time()\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint, early_stopping, tensorboard_callback]\n",
    "    )\n",
    "    end_time = time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    # print the training time\n",
    "    print('Training time: {}'.format(training_time))\n",
    "    \n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('Test Loss: {}'.format(loss))\n",
    "    print('Test Accuracy: {}'.format(accuracy))\n",
    "    \n",
    "    # predict the test set\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # y_pred = np.argmax(y_pred, axis=1)\n",
    "    # y_test = np.argmax(y_test, axis=1)\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)  # Convert probabilities to binary (0 or 1)\n",
    "    \n",
    "    # print the precision, recall and f1-score\n",
    "    model_precision = precision_score(y_test, y_pred)\n",
    "    model_recall = recall_score(y_test, y_pred)\n",
    "    model_f1_score = f1_score(y_test, y_pred)\n",
    "    print('Precision: {}'.format(model_precision))\n",
    "    print('Recall: {}'.format(model_recall))\n",
    "    print('F1-score: {}'.format(model_f1_score))\n",
    "        \n",
    "    # print the classification report\n",
    "    report = classification_report(y_test, y_pred, digits=4, target_names=['None_vishing', 'Vishing'])\n",
    "    print(report)\n",
    "    \n",
    "    # print the confusion matrix\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "    # save the model\n",
    "    # model.save('models/{}.h5'.format(model_name))\n",
    "    \n",
    "    # save the history\n",
    "    with open('histories/{}.pkl'.format(model_name), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    # save the results\n",
    "    df_results.loc[model_name] = [model_f1_score, model_precision, model_recall, accuracy, training_time]\n",
    "    # df_results.loc[model_name] = [f1_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), accuracy, training_time]  \n",
    "    \n",
    "    # print the results\n",
    "    # df_results\n",
    "    \n",
    "    return model, history, report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cda6204c1c043e69",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build, train and evaluate the LSTM model\n",
    "model_lstm, history_lstm, report_lstm = build_train_lstm_model('LSTM', X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2017516f93ef4f8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the results\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a0ca2bfe3338c02",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot the results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d41f51391066c0e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_classification_report(report_lstm, 'LSTM_Classification_report')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "625c0810be4efd9d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the training and validation loss and accuracy\n",
    "plot_loss_accuracy(history_lstm, 'LSTM')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c42da799e8a2e80",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plot the tensorboard results\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs/fit\n",
    "# %tensorboard --logdir logs/fit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95166221fd87fe2e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single LSTM layer with FastText embedding layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1caeff66dd045e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build, train and evaluate the LSTM model\n",
    "model_lstm_ft, history_lstm_ft, report_lstm_ft = build_train_lstm_model('LSTM_FT', X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test, embedding_matrix=embedding_matrix)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c831bd3c3b085ae",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the results\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd4b986a90fcc29a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_classification_report(report_lstm_ft, 'LSTM_FT_Classification_report')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2f9b8ad4eda3151",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the training and validation loss and accuracy\n",
    "plot_loss_accuracy(history_lstm_ft, 'LSTM_FT')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8b77062be8ac33f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stacked LSTM layers\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aba9dfd0ee4f6361"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# write a function to build, train and evaluate the LSTM model\n",
    "def build_train_stackedlstm_model(model_name, X_train, y_train, X_val, y_val, X_test, y_test, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, embedding_matrix=None):\n",
    "    \n",
    "    # define the model architecture\n",
    "    model = Sequential()\n",
    "    # check if the embedding_matrix is None or not\n",
    "    if embedding_matrix is None:\n",
    "        model.add(Embedding(max_words, embedding_dim, input_length=max_len, trainable=False))\n",
    "    else:\n",
    "        model.add(Embedding(max_words, embedding_dim, weights= [embedding_matrix], input_length=max_len, trainable=False))\n",
    "    \n",
    "    # model.add(SpatialDropout1D(spa_dropout_ratio))\n",
    "    model.add(LSTM(lstm_units_1, return_sequences=True))\n",
    "    model.add(LSTM(lstm_units_2))\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    # model.add(LSTM(lstm_units_1, dropout=dropout_ratio, recurrent_dropout=dropout_ratio))\n",
    "    model.add(Dropout(dropout_ratio))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "    \n",
    "    # compile the model\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        # optimizer=Adam(learning_rate=learning_rate, decay=learning_rate / decay_steps),\n",
    "        optimizer = Adam(learning_rate=lr_schedule),\n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # print the model summary\n",
    "    print(model.summary())\n",
    "    print('#' * 150)\n",
    "    \n",
    "    # Create a callback that saves the best model during validation\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        \"models/\" + model_name + \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".h5\",\n",
    "        monitor='val_accuracy', # val_loss or val_accuracy \n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='max' # max for val_accuracy, min for val_loss\n",
    "    )\n",
    "    # EarlyStopping to stop training when the validation loss has stopped improving\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        verbose = 1,\n",
    "        patience = 3, # Number of epochs with no improvement after which training will be stopped.\n",
    "        mode = 'min'\n",
    "    )\n",
    "\n",
    "    # TensorBoard for visualization\n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # train the model\n",
    "    start_time = time()\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint, early_stopping, tensorboard_callback]\n",
    "    )\n",
    "    end_time = time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    # print the training time\n",
    "    print('Training time: {}'.format(training_time))\n",
    "    \n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('Test Loss: {}'.format(loss))\n",
    "    print('Test Accuracy: {}'.format(accuracy))\n",
    "    \n",
    "    # predict the test set\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # y_pred = np.argmax(y_pred, axis=1)\n",
    "    # y_test = np.argmax(y_test, axis=1)\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)  # Convert probabilities to binary (0 or 1)\n",
    "    \n",
    "    # print the precision, recall and f1-score\n",
    "    model_precision = precision_score(y_test, y_pred)\n",
    "    model_recall = recall_score(y_test, y_pred)\n",
    "    model_f1_score = f1_score(y_test, y_pred)\n",
    "    print('Precision: {}'.format(model_precision))\n",
    "    print('Recall: {}'.format(model_recall))\n",
    "    print('F1-score: {}'.format(model_f1_score))\n",
    "        \n",
    "    # print the classification report\n",
    "    report = classification_report(y_test, y_pred, digits=4, target_names=['None_vishing', 'Vishing'])\n",
    "    print(report)\n",
    "    \n",
    "    # print the confusion matrix\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "    # save the model\n",
    "    # model.save('models/{}.h5'.format(model_name))\n",
    "    \n",
    "    # save the history\n",
    "    with open('histories/{}.pkl'.format(model_name), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    # save the results\n",
    "    df_results.loc[model_name] = [model_f1_score, model_precision, model_recall, accuracy, training_time]\n",
    "    # df_results.loc[model_name] = [f1_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), accuracy, training_time]  \n",
    "    \n",
    "    # print the results\n",
    "    # df_results\n",
    "    \n",
    "    return model, history, report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e57ec3571bc44f1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build, train and evaluate the stacked LSTM layers model\n",
    "model_stackedlstm, history_stackedlstm, report_stackedlstm = build_train_stackedlstm_model('stackedLSTM', X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6249804883bea896",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the results\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8390b4bf4d8d6b75",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_classification_report(report_stackedlstm, 'doubleLSTM_Classification_report')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "114dba2a27540cea",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the training and validation loss and accuracy\n",
    "plot_loss_accuracy(history_stackedlstm, 'stackedLSTM')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cc0e4d9b8db2b78",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stacked LSTM layer with FastText embedding layer\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd4bf5de70aad18d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build, train and evaluate the stacked LSTM layers model with FastText embedding layer\n",
    "model_stackedlstm_ft, history_stackedlstm_ft, report_stackedlstm_ft = build_train_stackedlstm_model('stackedLSTM_FT', X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test, embedding_matrix=embedding_matrix)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cba82e22db70417",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the results\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ac013f720a88e7c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "plot_classification_report(report_stackedlstm_ft, 'stackedLSTM_FT_Classification_report')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "453c8bd4ab21c03d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the training and validation loss and accuracy\n",
    "plot_loss_accuracy(history_stackedlstm_ft, 'stackedLSTM_FT')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c9ce8b183276be6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single BiLSTM layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d81668053c0331c9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to build, train and evaluate the BiLSTM model\n",
    "def build_train_bilstm_model(model_name, X_train, y_train, X_val, y_val, X_test, y_test, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, embedding_matrix=None):\n",
    "    \n",
    "    # define the model architecture\n",
    "    model = Sequential()\n",
    "    # check if the embedding_matrix is None or not\n",
    "    if embedding_matrix is None:\n",
    "        model.add(Embedding(max_words, embedding_dim, input_length=max_len, trainable=False))\n",
    "    else:\n",
    "        model.add(Embedding(max_words, embedding_dim, weights= [embedding_matrix], input_length=max_len, trainable=False))\n",
    "    \n",
    "    # model.add(SpatialDropout1D(spa_dropout_ratio))\n",
    "    model.add(Bidirectional(LSTM(lstm_units_1)))\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dropout(dropout_ratio))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "    \n",
    "    # compile the model\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        # optimizer=Adam(learning_rate=learning_rate, decay=learning_rate / decay_steps),\n",
    "        optimizer = Adam(learning_rate=lr_schedule),\n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # print the model summary\n",
    "    print(model.summary())\n",
    "    print('#' * 150)\n",
    "    \n",
    "    # Create a callback that saves the best model during validation\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        \"models/\" + model_name + \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".h5\",\n",
    "        # \"models/BiLSTM_\"+ datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".h5\",\n",
    "        monitor='val_accuracy', # val_loss or val_accuracy \n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='max' # max for val_accuracy, min for val_loss\n",
    "    )\n",
    "    # EarlyStopping to stop training when the validation loss has stopped improving\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        verbose = 1,\n",
    "        patience = 3, # Number of epochs with no improvement after which training will be stopped.\n",
    "        mode = 'min'\n",
    "    )\n",
    "\n",
    "    # TensorBoard for visualization\n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # train the model\n",
    "    start_time = time()\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint, early_stopping, tensorboard_callback]\n",
    "    )\n",
    "    end_time = time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    # print the training time\n",
    "    print('Training time: {}'.format(training_time))\n",
    "    \n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('Test Loss: {}'.format(loss))\n",
    "    print('Test Accuracy: {}'.format(accuracy))\n",
    "    \n",
    "    # predict the test set\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # y_pred = np.argmax(y_pred, axis=1)\n",
    "    # y_test = np.argmax(y_test, axis=1)\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)  # Convert probabilities to binary (0 or 1)\n",
    "    \n",
    "    # print the precision, recall and f1-score\n",
    "    model_precision = precision_score(y_test, y_pred)\n",
    "    model_recall = recall_score(y_test, y_pred)\n",
    "    model_f1_score = f1_score(y_test, y_pred)\n",
    "    print('Precision: {}'.format(model_precision))\n",
    "    print('Recall: {}'.format(model_recall))\n",
    "    print('F1-score: {}'.format(model_f1_score))\n",
    "        \n",
    "    # print the classification report\n",
    "    report = classification_report(y_test, y_pred, digits=4, target_names=['None_vishing', 'Vishing'])\n",
    "    print(report)\n",
    "    \n",
    "    # print the confusion matrix\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "    # save the model\n",
    "    # model.save('models/{}.h5'.format(model_name))\n",
    "    \n",
    "    # save the history\n",
    "    with open('histories/{}.pkl'.format(model_name), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    # save the results\n",
    "    df_results.loc[model_name] = [model_f1_score, model_precision, model_recall, accuracy, training_time]\n",
    "    # df_results.loc[model_name] = [f1_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), accuracy, training_time]  \n",
    "    \n",
    "    # print the results\n",
    "    # df_results\n",
    "    \n",
    "    return model, history, report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10f81b5925a0b1f1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build, train and evaluate the BiLSTM model\n",
    "model_bilstm, history_bilstm, report_bilstm = build_train_bilstm_model('BiLSTM', X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9edaf34fad9548",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the results\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e255abfe8d64e4f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "plot_classification_report(report_bilstm, 'BiLSTM_Classification_report')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e325f64cf221a87",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the training and validation loss and accuracy\n",
    "plot_loss_accuracy(history_bilstm, 'BiLSTM')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f15853e33f406f5e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single BiLSTM layer with FastText embedding layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1378386f859f9376"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build, train and evaluate the BiLSTM model with FastText embedding layer\n",
    "model_bilstm_ft, history_bilstm_ft, report_bilstm_ft = build_train_bilstm_model('BiLSTM_FT', X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test, embedding_matrix=embedding_matrix)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10a8ad2745658e79",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the results\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa40985a53025335",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "plot_classification_report(report_bilstm_ft, 'BiLSTM_FT_Classification_report')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92f5e5e6ce1ed095",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the training and validation loss and accuracy\n",
    "plot_loss_accuracy(history_bilstm_ft, 'BiLSTM_FT')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f295141d753556a4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stacked BiLSTM layers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bda6f2ca02c14e9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to build, train and evaluate the stacked BiLSTM model\n",
    "def build_train_stackedbilstm_model(model_name, X_train, y_train, X_val, y_val, X_test, y_test, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, embedding_matrix=None):\n",
    "    \n",
    "    # define the model architecture\n",
    "    model = Sequential()\n",
    "    # check if the embedding_matrix is None or not\n",
    "    if embedding_matrix is None:\n",
    "        model.add(Embedding(max_words, embedding_dim, input_length=max_len, trainable=False))\n",
    "    else:\n",
    "        model.add(Embedding(max_words, embedding_dim, weights= [embedding_matrix], input_length=max_len, trainable=False))\n",
    "    \n",
    "    # model.add(SpatialDropout1D(spa_dropout_ratio))\n",
    "    model.add(Bidirectional(LSTM(lstm_units_1,return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(lstm_units_2)))\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dropout(dropout_ratio))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "    \n",
    "    # compile the model\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        # optimizer=Adam(learning_rate=learning_rate, decay=learning_rate / decay_steps),\n",
    "        optimizer = Adam(learning_rate=lr_schedule),\n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # print the model summary\n",
    "    print(model.summary())\n",
    "    print('#' * 150)\n",
    "    \n",
    "    # Create a callback that saves the best model during validation\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        \"models/\" + model_name + \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".h5\",\n",
    "        # \"models/BiLSTM_\"+ datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".h5\",\n",
    "        monitor='val_accuracy', # val_loss or val_accuracy \n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='max' # max for val_accuracy, min for val_loss\n",
    "    )\n",
    "    # EarlyStopping to stop training when the validation loss has stopped improving\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        verbose = 1,\n",
    "        patience = 3, # Number of epochs with no improvement after which training will be stopped.\n",
    "        mode = 'min'\n",
    "    )\n",
    "\n",
    "    # TensorBoard for visualization\n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # train the model\n",
    "    start_time = time()\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint, early_stopping, tensorboard_callback]\n",
    "    )\n",
    "    end_time = time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    # print the training time\n",
    "    print('Training time: {}'.format(training_time))\n",
    "    \n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('Test Loss: {}'.format(loss))\n",
    "    print('Test Accuracy: {}'.format(accuracy))\n",
    "    \n",
    "    # predict the test set\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # y_pred = np.argmax(y_pred, axis=1)\n",
    "    # y_test = np.argmax(y_test, axis=1)\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)  # Convert probabilities to binary (0 or 1)\n",
    "    \n",
    "    # print the precision, recall and f1-score\n",
    "    model_precision = precision_score(y_test, y_pred)\n",
    "    model_recall = recall_score(y_test, y_pred)\n",
    "    model_f1_score = f1_score(y_test, y_pred)\n",
    "    print('Precision: {}'.format(model_precision))\n",
    "    print('Recall: {}'.format(model_recall))\n",
    "    print('F1-score: {}'.format(model_f1_score))\n",
    "        \n",
    "    # print the classification report\n",
    "    report = classification_report(y_test, y_pred, digits=4, target_names=['None_vishing', 'Vishing'])\n",
    "    print(report)\n",
    "    \n",
    "    # print the confusion matrix\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "    # save the model\n",
    "    # model.save('models/{}.h5'.format(model_name))\n",
    "    \n",
    "    # save the history\n",
    "    with open('histories/{}.pkl'.format(model_name), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    # save the results\n",
    "    df_results.loc[model_name] = [model_f1_score, model_precision, model_recall, accuracy, training_time]\n",
    "    # df_results.loc[model_name] = [f1_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), accuracy, training_time]  \n",
    "    \n",
    "    # print the results\n",
    "    # df_results\n",
    "    \n",
    "    return model, history, report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f79dec586d42447e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build, train and evaluate the stacked BiLSTM model\n",
    "model_stackedbilstm, history_stackedbilstm, report_stackedbilstm = build_train_stackedbilstm_model('stackedBiLSTM', X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a9ebd0d0a19b6e6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the results\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1647903d69f31441",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "plot_classification_report(report_stackedbilstm, 'stackedBiLSTM_Classification_report')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0dc8845b6355089",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the training and validation loss and accuracy\n",
    "plot_loss_accuracy(history_stackedbilstm, 'stackedBiLSTM')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d01f5ed87f5e052",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stacked BiLSTM layer with FastText embedding layer with FastText embedding layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b792de0f8c653b70"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build, train and evaluate the stacked BiLSTM model with FastText embedding layer\n",
    "model_stackedbilstm_ft, history_stackedbilstm_ft, report_stackedbilstm_ft = build_train_stackedbilstm_model('stackedBiLSTM_FT', X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test, embedding_matrix=embedding_matrix)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7431551ecc4052b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the results\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6577c6cbfdc80061",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "plot_classification_report(report_stackedbilstm_ft, 'stackedBiLSTM_FT_Classification_report')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3060a6c551616415",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the training and validation loss and accuracy\n",
    "plot_loss_accuracy(history_stackedbilstm_ft, 'stackedBiLSTM_FT')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "603af0c3545bb189",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1D CNN Layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "644138f74d12dc05"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to build, train and evaluate the stacked BiLSTM model\n",
    "def build_train_cnn_model(model_name, X_train, y_train, X_val, y_val, X_test, y_test, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, embedding_matrix=None):\n",
    "    \n",
    "    # define the model architecture\n",
    "    model = Sequential()\n",
    "    # check if the embedding_matrix is None or not\n",
    "    if embedding_matrix is None:\n",
    "        model.add(Embedding(max_words, embedding_dim, input_length=max_len, trainable=False))\n",
    "    else:\n",
    "        model.add(Embedding(max_words, embedding_dim, weights= [embedding_matrix], input_length=max_len, trainable=False))\n",
    "\n",
    "    model.add(SpatialDropout1D(spa_dropout_ratio))\n",
    "    model.add(Conv1D(num_filters, kernel_size, activation='relu', padding='valid'))\n",
    "    model.add(MaxPooling1D())\n",
    "    # model.add(GlobalMaxPooling1D()) #takes the maximum value over the time dimension from each feature map generated by the convolutional layers, Dimensionality Reduction, Reduces Overfitting\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Flatten()) # no need if used Global poooling\n",
    "    model.add(Dropout(dropout_ratio))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "    \n",
    "    # compile the model\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        # optimizer=Adam(learning_rate=learning_rate, decay=learning_rate / decay_steps),\n",
    "        optimizer = Adam(learning_rate=lr_schedule),\n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # print the model summary\n",
    "    print(model.summary())\n",
    "    print('#' * 150)\n",
    "    \n",
    "    # Create a callback that saves the best model during validation\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        \"models/\" + model_name + \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".h5\",\n",
    "        # \"models/BiLSTM_\"+ datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".h5\",\n",
    "        monitor='val_accuracy', # val_loss or val_accuracy \n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='max' # max for val_accuracy, min for val_lossmode='mn'\n",
    "    )\n",
    "    # EarlyStopping to stop training when the validation loss has stopped improving\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        verbose = 1,\n",
    "        patience = 3, # Number of epochs with no improvement after which training will be stopped.\n",
    "        mode = 'min'\n",
    "    )\n",
    "\n",
    "    # TensorBoard for visualization\n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # train the model\n",
    "    start_time = time()\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint, early_stopping, tensorboard_callback]\n",
    "    )\n",
    "    end_time = time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    # print the training time\n",
    "    print('Training time: {}'.format(training_time))\n",
    "    \n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('Test Loss: {}'.format(loss))\n",
    "    print('Test Accuracy: {}'.format(accuracy))\n",
    "    \n",
    "    # predict the test set\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # y_pred = np.argmax(y_pred, axis=1)\n",
    "    # y_test = np.argmax(y_test, axis=1)\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)  # Convert probabilities to binary (0 or 1)\n",
    "    \n",
    "    # print the precision, recall and f1-score\n",
    "    model_precision = precision_score(y_test, y_pred)\n",
    "    model_recall = recall_score(y_test, y_pred)\n",
    "    model_f1_score = f1_score(y_test, y_pred)\n",
    "    print('Precision: {}'.format(model_precision))\n",
    "    print('Recall: {}'.format(model_recall))\n",
    "    print('F1-score: {}'.format(model_f1_score))\n",
    "        \n",
    "    # print the classification report\n",
    "    report = classification_report(y_test, y_pred, digits=4, target_names=['None_vishing', 'Vishing'])\n",
    "    print(report)\n",
    "    \n",
    "    # print the confusion matrix\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "    # save the model\n",
    "    # model.save('models/{}.h5'.format(model_name))\n",
    "    \n",
    "    # save the history\n",
    "    with open('histories/{}.pkl'.format(model_name), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    \n",
    "    # save the results\n",
    "    df_results.loc[model_name] = [model_f1_score, model_precision, model_recall, accuracy, training_time]\n",
    "    # df_results.loc[model_name] = [f1_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), accuracy, training_time]  \n",
    "    \n",
    "    # print the results\n",
    "    # df_results\n",
    "    \n",
    "    return model, history, report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9d9ff8466157f3b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build, train and evaluate the CNN model\n",
    "model_cnn, history_cnn, report_cnn = build_train_cnn_model('CNN', X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81ecdbbbddbda051",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the results\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d8af77ae212f81f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#delete the last row in the results dataframe\n",
    "# df_results.drop(df_results.tail(1).index, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a00ba026a87d5654",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "plot_classification_report(report_cnn, 'CNN_Classification_report')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10decec3d4c35c30",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the training and validation loss and accuracy\n",
    "plot_loss_accuracy(history_cnn, 'CNN')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d62ba137865277",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1D CNN layer with FastText embedding layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25ece3f13710d321"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build, train and evaluate the CNN model with FastText embedding layer\n",
    "model_cnn_ft, history_cnn_ft, report_cnn_ft = build_train_cnn_model('CNN_FT', X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test, embedding_matrix=embedding_matrix)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76ce283fe86fe559",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the results\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9905c61d2601dc2d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "plot_classification_report(report_cnn_ft, 'CNN_FT_Classification_report')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b154446fe74de678",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the training and validation loss and accuracy\n",
    "plot_loss_accuracy(history_cnn_ft, 'CNN_FT')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d24ca6653996b3a0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1D CNN layer with multiple kernel sizes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "931052dd7304b432"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to build, train and evaluate the 1D CNN layer with multiple kernel sizes model\n",
    "def build_train_cnn_multiple_model(model_name, X_train, y_train, X_val, y_val, X_test, y_test, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, embedding_matrix=None):\n",
    "    \n",
    "    # define the model architecture\n",
    "    model = Sequential()\n",
    "    # check if the embedding_matrix is None or not\n",
    "    if embedding_matrix is None:\n",
    "        model.add(Embedding(max_words, embedding_dim, input_length=max_len, trainable=False))\n",
    "    else:\n",
    "        model.add(Embedding(max_words, embedding_dim, weights= [embedding_matrix], input_length=max_len, trainable=False))\n",
    "\n",
    "    model.add(SpatialDropout1D(spa_dropout_ratio))\n",
    "    model.add(Conv1D(num_filters, kernel_size, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(num_filters, kernel_size+1, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(num_filters, kernel_size+2, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D())\n",
    "    # model.add(GlobalMaxPooling1D()) #takes the maximum value over the time dimension from each feature map generated by the convolutional layers, Dimensionality Reduction, Reduces Overfitting\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Flatten()) # no need if used Global poooling\n",
    "    model.add(Dropout(dropout_ratio))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "\n",
    "    # compile the model\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=learning_rate,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        # optimizer=Adam(learning_rate=learning_rate, decay=learning_rate / decay_steps),\n",
    "        optimizer=Adam(learning_rate=lr_schedule),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # print the model summary\n",
    "    print(model.summary())\n",
    "    print('#' * 150)\n",
    "\n",
    "    # Create a callback that saves the best model during validation\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        \"models/\" + model_name + \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".h5\",\n",
    "        # \"models/BiLSTM_\"+ datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".h5\",\n",
    "        monitor='val_accuracy',  # val_loss or val_accuracy \n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='max'  # max for val_accuracy, min for val_lossmode='mn'\n",
    "    )\n",
    "    # EarlyStopping to stop training when the validation loss has stopped improving\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        patience=3,  # Number of epochs with no improvement after which training will be stopped.\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    # TensorBoard for visualization\n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # train the model\n",
    "    start_time = time()\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint, early_stopping, tensorboard_callback]\n",
    "    )\n",
    "    end_time = time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    # print the training time\n",
    "    print('Training time: {}'.format(training_time))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('Test Loss: {}'.format(loss))\n",
    "    print('Test Accuracy: {}'.format(accuracy))\n",
    "\n",
    "    # predict the test set\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # y_pred = np.argmax(y_pred, axis=1)\n",
    "    # y_test = np.argmax(y_test, axis=1)\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)  # Convert probabilities to binary (0 or 1)\n",
    "\n",
    "    # print the precision, recall and f1-score\n",
    "    model_precision = precision_score(y_test, y_pred)\n",
    "    model_recall = recall_score(y_test, y_pred)\n",
    "    model_f1_score = f1_score(y_test, y_pred)\n",
    "    print('Precision: {}'.format(model_precision))\n",
    "    print('Recall: {}'.format(model_recall))\n",
    "    print('F1-score: {}'.format(model_f1_score))\n",
    "\n",
    "    # print the classification report\n",
    "    report = classification_report(y_test, y_pred, digits=4, target_names=['None_vishing', 'Vishing'])\n",
    "    print(report)\n",
    "\n",
    "    # print the confusion matrix\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # save the model\n",
    "    # model.save('models/{}.h5'.format(model_name))\n",
    "\n",
    "    # save the history\n",
    "    with open('histories/{}.pkl'.format(model_name), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    # save the results\n",
    "    df_results.loc[model_name] = [model_f1_score, model_precision, model_recall, accuracy, training_time]\n",
    "    # df_results.loc[model_name] = [f1_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), accuracy, training_time]  \n",
    "\n",
    "    # print the results\n",
    "    # df_results\n",
    "\n",
    "    return model, history, report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbc126155546835b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build, train and evaluate the CNN with mulitple kernel sizes model\n",
    "model_cnn_multiple, history_cnn_multiple, report_cnn_multiple = build_train_cnn_multiple_model('CNN_multiple', X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cffc64605784bbe6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the results\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "193a58ba2acd26a7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "plot_classification_report(report_cnn_multiple, 'CNN_multiple_Classification_report')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae4a657cf8d221cf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the training and validation loss and accuracy\n",
    "plot_loss_accuracy(history_cnn_multiple, 'CNN_multiple')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f01907d1c8d0c472",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1D CNN layer with multiple kernel sizes with FastText embedding layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d004131060eb0d92"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build, train and evaluate the CNN with multiple kernel sizes model with FastText embedding layer\n",
    "model_cnn_multiple_ft, history_cnn_multiple_ft, report_cnn_multiple_ft = build_train_cnn_multiple_model('CNN_multiple_FT', X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test, embedding_matrix=embedding_matrix)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7546a459ef600855",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the results\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "110f795e7550352d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "plot_classification_report(report_cnn_multiple_ft, 'CNN_multiple_FT_Classification_report')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "240639395d76b2a7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the training and validation loss and accuracy\n",
    "plot_loss_accuracy(history_cnn_multiple_ft, 'CNN_multiple_FT')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3f4fbf4c4f72a86",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1D CNN layer with multiple kernel sizes version 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c48bc9607f85dd1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to build, train and evaluate the 1D CNN layer with multiple kernel sizes model\n",
    "def build_train_cnn_multiple_kernelsv2_model(model_name, X_train, y_train, X_val, y_val, X_test, y_test, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, embedding_matrix=None):\n",
    "    \n",
    "    \n",
    "    # Define the input\n",
    "    input_layer = Input(shape=(max_len,))\n",
    "    \n",
    "    # Embedding layer\n",
    "   # check if the embedding_matrix is None or not\n",
    "    if embedding_matrix is None:\n",
    "        # model.add(Embedding(max_words, embedding_dim, input_length=max_len, trainable=False))\n",
    "        embedding_layer = Embedding(max_words, embedding_dim, input_length=max_len, trainable=False)(input_layer)\n",
    "    else:\n",
    "        # model.add(Embedding(max_words, embedding_dim, weights= [embedding_matrix], input_length=max_len, trainable=False))\n",
    "        embedding_layer = Embedding(max_words, embedding_dim, weights= [embedding_matrix], input_length=max_len, trainable=False)(input_layer)\n",
    "    \n",
    "    # Define multiple convolutional layers with different kernel sizes\n",
    "    conv_layers = []\n",
    "    kernel_sizes = [1, 2, 3, 5]\n",
    "    for kernel_size in kernel_sizes:\n",
    "        conv_layer = Conv1D(filters=num_filters, kernel_size=kernel_size, padding=\"valid\", activation='relu')(embedding_layer)\n",
    "        # conv_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "        conv_layer = MaxPooling1D()(conv_layer)\n",
    "        conv_layer = Flatten()(conv_layer)\n",
    "        conv_layers.append(conv_layer)\n",
    "    \n",
    "    # Concatenate the outputs of the convolutional layers, Concatenate all convolutional layers\n",
    "    concat_layer = Concatenate()(conv_layers)\n",
    "    \n",
    "    # Add a dense layer\n",
    "    dense_layer = Dense(dense_units, activation='relu')(concat_layer)\n",
    "    if dropout_ratio > 0:\n",
    "        dense_layer = Dropout(dropout_ratio)(dense_layer)\n",
    "    \n",
    "    # Add the output layer\n",
    "    output_layer = Dense(1, activation='sigmoid')(dense_layer)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    # \n",
    "    # # define the model architecture\n",
    "    # model = Sequential()\n",
    "    # # check if the embedding_matrix is None or not\n",
    "    # if embedding_matrix is None:\n",
    "    #     model.add(Embedding(max_words, embedding_dim, input_length=max_len, trainable=False))\n",
    "    # else:\n",
    "    #     model.add(Embedding(max_words, embedding_dim, weights= [embedding_matrix], input_length=max_len, trainable=False))\n",
    "    # \n",
    "    # model.add(SpatialDropout1D(spa_dropout_ratio))\n",
    "    # model.add(Conv1D(num_filters, kernel_size, activation='relu', padding='same'))\n",
    "    # model.add(MaxPooling1D())\n",
    "    # model.add(Conv1D(num_filters, kernel_size+1, activation='relu', padding='same'))\n",
    "    # model.add(MaxPooling1D())\n",
    "    # model.add(Conv1D(num_filters, kernel_size+2, activation='relu', padding='same'))\n",
    "    # model.add(MaxPooling1D())\n",
    "    # # model.add(GlobalMaxPooling1D()) #takes the maximum value over the time dimension from each feature map generated by the convolutional layers, Dimensionality Reduction, Reduces Overfitting\n",
    "    # model.add(Dense(dense_units, activation='relu'))\n",
    "    # model.add(Flatten()) # no need if used Global poooling\n",
    "    # model.add(Dropout(dropout_ratio))\n",
    "    # model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
    "\n",
    "    # compile the model\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=learning_rate,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        # optimizer=Adam(learning_rate=learning_rate, decay=learning_rate / decay_steps),\n",
    "        optimizer=Adam(learning_rate=lr_schedule),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # print the model summary\n",
    "    print(model.summary())\n",
    "    print('#' * 150)\n",
    "\n",
    "    # Create a callback that saves the best model during validation\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        \"models/\" + model_name + \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".h5\",\n",
    "        # \"models/BiLSTM_\"+ datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".h5\",\n",
    "        monitor='val_accuracy',  # val_loss or val_accuracy \n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='max'  # max for val_accuracy, min for val_lossmode='mn'\n",
    "    )\n",
    "    # EarlyStopping to stop training when the validation loss has stopped improving\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        patience=3,  # Number of epochs with no improvement after which training will be stopped.\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    # TensorBoard for visualization\n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # train the model\n",
    "    start_time = time()\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint, early_stopping, tensorboard_callback]\n",
    "    )\n",
    "    end_time = time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    # print the training time\n",
    "    print('Training time: {}'.format(training_time))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('Test Loss: {}'.format(loss))\n",
    "    print('Test Accuracy: {}'.format(accuracy))\n",
    "\n",
    "    # predict the test set\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # y_pred = np.argmax(y_pred, axis=1)\n",
    "    # y_test = np.argmax(y_test, axis=1)\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)  # Convert probabilities to binary (0 or 1)\n",
    "\n",
    "    # print the precision, recall and f1-score\n",
    "    model_precision = precision_score(y_test, y_pred)\n",
    "    model_recall = recall_score(y_test, y_pred)\n",
    "    model_f1_score = f1_score(y_test, y_pred)\n",
    "    print('Precision: {}'.format(model_precision))\n",
    "    print('Recall: {}'.format(model_recall))\n",
    "    print('F1-score: {}'.format(model_f1_score))\n",
    "\n",
    "    # print the classification report\n",
    "    report = classification_report(y_test, y_pred, digits=4, target_names=['None_vishing', 'Vishing'])\n",
    "    print(report)\n",
    "\n",
    "    # print the confusion matrix\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # save the model\n",
    "    # model.save('models/{}.h5'.format(model_name))\n",
    "\n",
    "    # save the history\n",
    "    with open('histories/{}.pkl'.format(model_name), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    # save the results\n",
    "    df_results.loc[model_name] = [model_f1_score, model_precision, model_recall, accuracy, training_time]\n",
    "    # df_results.loc[model_name] = [f1_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), accuracy, training_time]  \n",
    "\n",
    "    # print the results\n",
    "    # df_results\n",
    "\n",
    "    return model, history, report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cab94c492ad1fc77",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build, train and evaluate the CNN with multiple kernel sizes model\n",
    "model_cnn_multiple_v2, history_cnn_multiple_v2, report_cnn_multiple_v2 = build_train_cnn_multiple_kernelsv2_model('CNN_1235ker', X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f65a5745463f94e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the results\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "929492771b1802a3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "plot_classification_report(report_cnn_multiple_v2, 'CNN_1235ker_Classification_report')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ed5a5b6c042e7f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the training and validation loss and accuracy\n",
    "plot_loss_accuracy(history_cnn_multiple_v2, 'CNN_1235ker')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10713f81d577a91b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1D CNN layer with multiple kernel sizes version 2 with FastText embedding layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79dd35af4bfa76d5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# build, train and evaluate the CNN with multiple kernel sizes model with FastText embedding layer\n",
    "model_cnn_multiple_v2_ft, history_cnn_multiple_v2_ft, report_cnn_multiple_v2_ft = build_train_cnn_multiple_kernelsv2_model('CNN_1235ker_FT', X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test, embedding_matrix=embedding_matrix)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e24b62eaf6663d27"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print the results\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69028d51d1745bb9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "plot_classification_report(report_cnn_multiple_v2_ft, 'CNN_1235ker_FT_Classification_report')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7b83001ca86cc0f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot the training and validation loss and accuracy\n",
    "plot_loss_accuracy(history_cnn_multiple_v2_ft, 'CNN_1235ker_FT')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2adb09c630010691"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## save the results in csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4e92f6ad018a254"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save the training results\n",
    "df_results.to_csv(\"reports/Models_performance_summary_\" + datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + \".csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7037ddc7157dbb8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot all the models performance\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81d2cbfe59f8ff0a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # plot overall accuracy on test set\n",
    "# fig = plt.figure(figsize=(9,6))\n",
    "# plt.plot(history_2.epoch, history_2.history['val_accuracy'], '-o', label='1D CNN')\n",
    "# plt.plot(history_3.epoch, history_3.history['val_accuracy'], '-o', label='LSTM')\n",
    "# plt.plot(history_4.epoch, history_4.history['val_accuracy'], '-o', label='BiLSTM')\n",
    "# plt.plot(history_5.epoch, history_5.history['val_accuracy'], '-o', label='CNN-BiLSTM')\n",
    "# plt.plot(history_1.epoch, history_1.history['val_accuracy'], '-o', label='Proposed')\n",
    "# plt.title('Validation Accuracy of All Models')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.xlim(left=0)\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.savefig('reports/All_models_accuracy_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "# plt.savefig('reports/All_models_accuracy_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "# plt.show()\n",
    "# plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d78384625e1f3c4b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # plot overall looss on test set\n",
    "# fig = plt.figure(figsize=(9,6))\n",
    "# plt.plot(history_2.epoch, history_2.history['val_loss'], '-o', label='1D CNN')\n",
    "# plt.plot(history_3.epoch, history_3.history['val_loss'], '-o', label='LSTM')\n",
    "# plt.plot(history_4.epoch, history_4.history['val_loss'], '-o', label='BiLSTM')\n",
    "# plt.plot(history_5.epoch, history_5.history['val_loss'], '-o', label='CNN-BiLSTM')\n",
    "# plt.plot(history_1.epoch, history_1.history['val_loss'], '-o', label='Proposed')\n",
    "# plt.title('Validation Loss of All Models')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.xlim(left=0)\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.savefig('reports/All_models_val_loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "# plt.savefig('reports/All_models_val_loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "# plt.show()\n",
    "# plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efb5f582b134032e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
