{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OM4ByVuU9r6R",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Korean Text classification with KoBERT</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q30zE2Xy-ipp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Summary of the project**\n",
    "\n",
    "In Korea, even though there are many research conducted being conducted on voice phishing, it remains a real case problem that technology such as artificial intelligence can tackle. Through a previous project conducted, we created a dataset containing phone call conversation transcripts and general conversation text data. This voice phishing dataset has two different class which are **voice phishing** (represented as \"1\") and **non-voice phishing** (represented as \"0\").\n",
    "\n",
    "Using this dataset with state-of-the-art (SOTA) pre-trained word embedding [KoBERT](https://github.com/SKTBrain/KoBERT), we will perform NLP task such as text classification to build binary classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z48RIrXt-27-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Aim of the project**\n",
    "In this project, we aim to build binary classification models capable to determine whether the inputted Korean conversation text is voice phishing (\"1\") or non-voice phishing (\"0\") related text.\n",
    "\n",
    "The API used are Tensorflow for BERT model and Pytorch for KoBERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZWbz7tr_D4e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Desired outputs of the project**\n",
    "From the trained models, we expect to achieve great classification performance on this voice phishing dataset such as the model tells us if a conversation is harmful or not harmful.\n",
    "At the end of this project, we will look at the accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-U5oXwtUIUZx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training the binary classification model with KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMY5U0umGrhJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Installing the common needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SygClswK-r1",
    "outputId": "c385330a-d2e0-425f-8bc4-a0d127f6e54a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-ddiszrb2\n",
      "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-ddiszrb2\n",
      "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit a82d428c26988ff40c03309038dba813fb83b92e\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: boto3 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from kobert==0.2.3) (1.20.34)\n",
      "Requirement already satisfied: gluonnlp>=0.6.0 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from kobert==0.2.3) (0.10.0)\n",
      "Requirement already satisfied: mxnet>=1.4.0 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from kobert==0.2.3) (1.9.0)\n",
      "Requirement already satisfied: onnxruntime==1.8.0 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from kobert==0.2.3) (1.8.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.6 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from kobert==0.2.3) (0.1.96)\n",
      "Requirement already satisfied: torch>=1.7.0 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from kobert==0.2.3) (1.8.2+cu111)\n",
      "Requirement already satisfied: transformers>=4.8.1 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from kobert==0.2.3) (4.17.0)\n",
      "Requirement already satisfied: flatbuffers in /home/phenomx/anaconda3/lib/python3.8/site-packages (from onnxruntime==1.8.0->kobert==0.2.3) (1.12)\n",
      "Requirement already satisfied: protobuf in /home/phenomx/anaconda3/lib/python3.8/site-packages (from onnxruntime==1.8.0->kobert==0.2.3) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from onnxruntime==1.8.0->kobert==0.2.3) (1.19.2)\n",
      "Requirement already satisfied: cython in /home/phenomx/anaconda3/lib/python3.8/site-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (0.29.21)\n",
      "Requirement already satisfied: packaging in /home/phenomx/anaconda3/lib/python3.8/site-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (20.9)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from mxnet>=1.4.0->kobert==0.2.3) (2.25.1)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from mxnet>=1.4.0->kobert==0.2.3) (0.8.4)\n",
      "Requirement already satisfied: typing-extensions in /home/phenomx/anaconda3/lib/python3.8/site-packages (from torch>=1.7.0->kobert==0.2.3) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /home/phenomx/anaconda3/lib/python3.8/site-packages (from transformers>=4.8.1->kobert==0.2.3) (3.0.12)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from transformers>=4.8.1->kobert==0.2.3) (0.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from transformers>=4.8.1->kobert==0.2.3) (0.4.0)\n",
      "Requirement already satisfied: sacremoses in /home/phenomx/anaconda3/lib/python3.8/site-packages (from transformers>=4.8.1->kobert==0.2.3) (0.0.47)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from transformers>=4.8.1->kobert==0.2.3) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from transformers>=4.8.1->kobert==0.2.3) (2020.11.13)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from transformers>=4.8.1->kobert==0.2.3) (4.36.1)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from boto3->kobert==0.2.3) (0.5.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.34 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from boto3->kobert==0.2.3) (1.23.34)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from boto3->kobert==0.2.3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from botocore<1.24.0,>=1.23.34->boto3->kobert==0.2.3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from botocore<1.24.0,>=1.23.34->boto3->kobert==0.2.3) (1.26.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from packaging->gluonnlp>=0.6.0->kobert==0.2.3) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2.10)\n",
      "Requirement already satisfied: six>=1.9 in /home/phenomx/anaconda3/lib/python3.8/site-packages (from protobuf->onnxruntime==1.8.0->kobert==0.2.3) (1.15.0)\n",
      "Requirement already satisfied: click in /home/phenomx/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/phenomx/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (1.0.0)\n",
      "\u001B[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/home/phenomx/anaconda3/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "# dowload and install KoBERT as a python package\n",
    "# this commande will install the requirted package at the same time\n",
    "  # gluonnlp >= 0.6.0\n",
    "  # mxnet >= 1.4.0\n",
    "  # onnxruntime >= 0.3.0\n",
    "  # sentencepiece >= 0.1.6\n",
    "  # torch >= 1.7.0\n",
    "  # transformers >= 4.8.1\n",
    "\n",
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl-QdJx5O30T",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import all the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "kZM6H2ZhM4rE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## importing the required packages\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "bFFUZg92HcCj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "G3MdTD7jN7zN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## importing KoBERT functions\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "-wrZ2zmpOfEx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## import transformers functions\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "uLMyT454u9KP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Configure the GPU  device\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfQAKaHmTX9v",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ok-GzRAgTd28",
    "outputId": "59326451-c380-4a5e-cb4f-2c74d7dacb8c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Since we are using Colab, we will provide a test to check if environment is \n",
    "colab or not so that the data can also be imported in case this jupyter file is \n",
    "ran on local machine and not on colab\n",
    "\"\"\"\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on CoLab')\n",
    "  ## mount the google drive\n",
    "  from google.colab import drive\n",
    "  drive.mount('drive')\n",
    "  # move to the directory where dataset is saved\n",
    "  %cd drive/My\\ Drive/Colab\\ Notebooks/\n",
    "else:\n",
    "  print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "vdZBdvWvnbfB",
    "outputId": "e03a5fc6-504b-45dd-a4aa-7d78ba71d2b8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transcript</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>그거 그쪽 카드 통합 채권 관리 부서 연람 게끔 어제 사장 전화 동의 부분 전산 확...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>그것 나쁘 방법 사실 연봉 얼마나 근무 환경 얘기 잖아 되게 다더라 아니 그런 얘기...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>본점 으로 서류 보내 드릴 에요 네네 그러면 고객 대출금 승인 오늘 저희 으로 본점...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>수고 십니다 서울 지검 수사관 에요 지금 통화 괜찮 으세요 지금 다름 아니 으로 명...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>혹시나 해서 그냥 금융 감독원 공부 요청 해서 회원 확실히 연락 드렸 오늘 입금 처...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>많이 드라 명훈 자기 에서 160 불렀 쩌쪽 인제 진영 에서 140 불렀 거든 근데...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>약간 헬스 직원 에서 만약 누가 면은 전달 드리 아니 어쩔 최선 잖아 어떻게 무슨 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>왜냐면 고대 해도 고대 부터 중세 까지 해도 철학 되게 그니까 인문학 되게 받들 여...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>그리고 와이파이 터진다고 노트북 때릴 기세 와이파이 터진다고 약간 그런 무슨 자기 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>아까 전화 드렸 정말 주사 과정 대해서 녹취 상태 다시 총괄 책임 드릴 예요 아니요...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>증거 속이 증거 자료 솔직히 어려운 부분 조금 으신데 혹시 지금 통화 계속 장소 어...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>왜냐면 맨날 다는 얘기 그래 자기 병원 간다는 얘기 아니 여도 문젠 그럼 그런 노력...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>근데 그래도 우리 인가 되게 상위 유지 거든 까진 아니 어도 그서 그냥 그냥 묻혀 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>딱히 속눈썹 붙이 거나 네일 버리 니까는 12 내내 동안 어서 오래 집안일 많이 니...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>주세요 때문 그거 저희 지금 전화 김용민 지금 불법 도박 사이트 지금 필리핀 에서 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Transcript  Label\n",
       "660   그거 그쪽 카드 통합 채권 관리 부서 연람 게끔 어제 사장 전화 동의 부분 전산 확...      1\n",
       "1150  그것 나쁘 방법 사실 연봉 얼마나 근무 환경 얘기 잖아 되게 다더라 아니 그런 얘기...      0\n",
       "1013  본점 으로 서류 보내 드릴 에요 네네 그러면 고객 대출금 승인 오늘 저희 으로 본점...      1\n",
       "893   수고 십니다 서울 지검 수사관 에요 지금 통화 괜찮 으세요 지금 다름 아니 으로 명...      1\n",
       "480   혹시나 해서 그냥 금융 감독원 공부 요청 해서 회원 확실히 연락 드렸 오늘 입금 처...      1\n",
       "953   많이 드라 명훈 자기 에서 160 불렀 쩌쪽 인제 진영 에서 140 불렀 거든 근데...      0\n",
       "443   약간 헬스 직원 에서 만약 누가 면은 전달 드리 아니 어쩔 최선 잖아 어떻게 무슨 ...      0\n",
       "1081  왜냐면 고대 해도 고대 부터 중세 까지 해도 철학 되게 그니까 인문학 되게 받들 여...      0\n",
       "398   그리고 와이파이 터진다고 노트북 때릴 기세 와이파이 터진다고 약간 그런 무슨 자기 ...      0\n",
       "203   아까 전화 드렸 정말 주사 과정 대해서 녹취 상태 다시 총괄 책임 드릴 예요 아니요...      1\n",
       "1169  증거 속이 증거 자료 솔직히 어려운 부분 조금 으신데 혹시 지금 통화 계속 장소 어...      1\n",
       "1110  왜냐면 맨날 다는 얘기 그래 자기 병원 간다는 얘기 아니 여도 문젠 그럼 그런 노력...      0\n",
       "1005  근데 그래도 우리 인가 되게 상위 유지 거든 까진 아니 어도 그서 그냥 그냥 묻혀 ...      0\n",
       "624   딱히 속눈썹 붙이 거나 네일 버리 니까는 12 내내 동안 어서 오래 집안일 많이 니...      0\n",
       "863   주세요 때문 그거 저희 지금 전화 김용민 지금 불법 도박 사이트 지금 필리핀 에서 ...      1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "dataset = pd.read_csv('KorCCViD_v1.3_fullcleansed.csv').sample(frac=1.0)\n",
    "dataset.sample(n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-P7hdv9mdt6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data transformation and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "bB_0ae0MkgOT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## transform our train set and test set into tsv file to usedd into KoBERT\n",
    "# train_tsv = nlp.data.TSVDataset('KorCCViD_v1.3_fullcleansed.csv')\n",
    "# train_tsv = nlp.data.TSVDataset('KorCCViD_v1.3_fullcleansed.csv')\n",
    "\n",
    "dataset_tsv = []\n",
    "for text, label in zip(dataset['Transcript'], dataset['Label']):\n",
    "    data = []\n",
    "    data.append(text)\n",
    "    data.append(str(label))\n",
    "\n",
    "    dataset_tsv.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UXD_FjO2qXrU",
    "outputId": "40720195-8f80-4ea8-85d5-77c35ed314a7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['여보세요 입금 아까 계좌 오류 나가 오류 잠시 잠시 만요 오류 나온다 고요 금융 감독원 에서 발급 요청 해서 고객 께서 에스크로 계좌 발급 신청 아요 그렇 때문 문제 저희 에서 확인 잠시 그러 일단 고객 납부 납부 시간 너무 지연 어요 에스크로 계좌 경우 납부 다고 지연 시간 너무 오래 걸리 면은 다시 회수 처리 거든요 아마 지금 현재 시간 너무 오래 때문 입금 예요 그죠 입금 오류 나오 아요 그래서 다시 발급 요청 해야 돼요 그래서 일단 직장 근무 셔야 때문 일단 복귀 세요 복귀 산와 대부 산와 머니 통화 해서 다시 해서 에스크로 계좌 발급 요청 할께요 그런데 발급 요청 저희 발급 더라고 해서 바로 나오 아니 거든요 에스크로 계좌 경우 금융 감독원 에서 안전 진행 드리 위해서 고객 께서 만일 저희 에서 상대 산와 머니 납부 중도 상환 없이 했는데도 불구 저희 에서 만일 대출 부결 부결 다시 환급 계좌 발급 에요 그렇 때문 지금 시간 오래 지연 잖아요 그래서 산와 머니 에서 통화 해서 에스크로 계좌 발급 새로 발급 드리 전화 드리 습니다 근데 어서 통화 어려울 거든요 그러 세요 그러면 그러면요 어떻게 오늘 지금 시간 아요 그래서 시간 여섯 저희 금융 업무 마감 시간 때문 오늘 납부 난다 더라도 내일 내일 해서 송금 처리 으실 거든요 시간 너무 촉박 니까 그래서 내일 오전 으로 처리 어떠실까 편하 시간 그게 괜찮 아요 그렇 습니다 그러면 내일 오전 정도 해서 진행 드릴까요 그럼 12 아니 오후 정도 그때 오후 오전 오후 에서 사이 해서 진행 드릴게요 습니다 그럼 오늘 근무 구요 괜찮 내일 저희 대출금 수령 게끔 처리 건데 괜찮 내일 해도 상관 어요 습니다 고객',\n",
       "  '1'],\n",
       " ['그래 어떤 정말로 맛있 예요 모두 보여 드릴게요 지금 그래서 저희 과정 에서 통장 일단 당하 에게 걸로 확인 고요 확인 연락 드립니다 당하 셔서 계신 건지 직접 건지 확인 세요 아니 면서 200 아니 중요 저희 수업 어요 니까 사람 30 에서 50 까지 대창 시대 부분 에요 저희 합니까 통화 그래서 아니 거든요 고모 지금 그리고 에서 그렇게 계시 지요 해서 항공 침대 물건 학원 에요 아까 말씀 드린 통장 으로 많이 상태 고요 오늘 야근 니까 상태 입니다 그래서 일단 원치 때문 관련 사람 라고 구경 언니 오늘 엄청 심한 통장 갠가 거든요 옛날 어렸 만든 만들 다시 들어왔 그런 그런 아니 니까 시간 그런지 만들 느냐 2015 12 확인 라고 바로 경비실 시켰 어요 사장 드릴게요 말씀 드릴게요 통장 잖아요 출근 목요일 치킨 드리 됩니다 때문 우선 으로 때문 그거 확인 부분 아니 고요 드릴게요 그래서 지금 얘기 때문 저희 다니 면서 예요 따라서 누르 고요 저희 에서 법원 제출 한복 통해서 국민카드 잠겨서 냐면 그거 겠죠 통장 거래 내역 확인 예요 지금 출장 관련 해서 여쭤 부분 지금 실례 지만 지금 어디 근무 신가요 아니면 갑자기 시간 신가 계시 지금 부터 통화 요금 해서 그래요 그럼 지금 단단 나빠 으니까 말씀 됩니다 다른 내용 말씀 니까 그리 말씀 그럼 지금 부터 공항 도착 습니다 거나 모르 아니 아니 아니야 2015 12 통장 그때 라이프 부터 까지 수영장 어요 송장 등록 습니다 동의 십니까 사이 지갑 신분증 보내 확인 물건 습니까 현재 위치 에서 사용 예요 말씀 드려서 건데요 자동차 사람 위해서 간단 지금 퇴근 해요 그러면 어쩌면 사용 세요 월급 들어오 성장 월급 들어오 종각역 에서 거기 카드 옮겨 그래서 지금 나요 소리 들리 그래요 그러면 십니까 다시 다시 진행 해야 돼요 지금 아니 괜찮 아요 지금 사람 피해 보상 계세요 자전거 근데 잖아요 시간 얘기 해서 예요 전부 그런 해서 누구 차후 마찬가지 예요 그냥 나왔 라면 셔야 지금 내려 갈게요 남성 김현철 구나 습니다 십니까 지금 만나 얘기 예요 확인 건데요 사랑 그러 면은 지금 기술 오늘 다시 근데 그거 했었 우리 우리 네요 그냥 그러면 신한은행 경우 20 30 10 20 모으 카드 라서 지금 20 아요 그러면 지금 까지 모드 실행 많이 지금 부터 녹음 해서 우리 예요 만약 어서 경우 입장 그게 아니 잖아요 때문 다시 연락 드릴 예요 끝날 예요 그래서 다시 라고 메모 연락 신다고 보내 드릴 예요 한번 확인 어떤 확인 갈게요 해서 직원 나왔 거든요 날까 등기부 확인 해서 보내 드릴 예요 한번 확인 부분 으시 예요 아요 금방 거기 갑자기 생각난 하나 생각난 건데 2009 부터 10 까지 사진 남자 친구 거든요 근데 사람 어요 사람 라고요 그러 모르 근데 근데 사람 어요 오늘 옛날 그때 핸드폰 개통 그거 어렵 생각 몰랐 거의 거든요 근데 어떻게 저장 놨는데 모르 사람 생각 네요 친구 그런 어요 그래서 하여튼 조금 어서 아야지 무서운 사람 고요 어떤 인지 해도 니까 걱정 마시 고요 근데 엄청 엄청 엄청 괴롭힐 그래 지금 엄청 무서운 사람 들어와서 다시 마세요 으니까 마시 고요 니까 사람 어요 어떻게 예요 확인 알려 으시 예요 걱정 마시 오늘',\n",
       "  '1'],\n",
       " ['그녀 거짓말 사랑 그거 영화 거든 근데 영화 췌장 스토리 대충 전혀 다른 그녀 거짓말 사랑 근데 면서 솔직히 불편 더라고 여자 주인공 너무 바보 남자 주인공 생겨서 노래 그래서 암튼 다른 라는',\n",
       "  '0'],\n",
       " ['오늘 15 김용수 금융 사기 일당 으면 상관 현장 에서 지금 대량 대포통장 체크카드 으로 사람 로뎀 농협 기업 통장 같이 발견 습니다 그래서 지금 사실 확인 연락 드리 통장 보문 통해서 알아본 결과 지금 70 불법 자금 많이 계속 빠져나갔 나머지 잔액 천만 지금 결제 상태 입니다 으시 고요 사용 안함 그래요 통장 14 10 17 영등포구 당산 지점 에서 본인 명의 걸쳐서 계속 습니다 혹시 그래요 오늘 계좌 본인 직접 아니면은 본인 어떤 개인 정보 주일 어서 도형 연락 드린다고 말씀 서울 중앙 지방 검찰청 금융 범죄 수사 박철호 돌아갑니다 그래서 선생 께서 온라인 소환 조사 진행 습니다 아직 까지 사건 피해자 인지 인지 확인 판단 어렵 때문 대부분 계좌 라고 생각 일단 전화 드립니다 하지만 일단 신고 계시 때문 통장 으로 보내 어쩔 김용술 당하 사기 당한 저기 나라 사이트 라도 물건 보내 물건 보내 보내 대포통장 개설 내용 입니다 대부분 본인 아니 지금 200 정도 됩니다 전화 해서 조사 어떤 통장 바라 먼저 지금 계십니다 혹시 니까 통장 발급 양심 으로 본인 어떤 개인 정보 유출 통장 만들 면은 그대로 차원 차원 으로 금전 피해 습니다 왜냐하면 사람 김용수 농협 출신 대부분 카드 에서 하나 금융업 종사 사람 아기 정도 잡지 못하 지금 거지 모르 상태 입니다 그래서 계속 조사 진행 고요 일단 어쩔 지만 만약 조사 통해서 면은 전화 진행 온라인 소환 조사 중지 관계 소환 조사 어야 됩니다 하지만 연관 자신 라면 어쩌 불미 스러운 사건 책임 지지 셔도 고요 그리고 저희 본인 으로 기회 입장 확인서 보내 드릴 겁니다 그러면 그걸로 해당 지역 구청 민원 보상 라고 습니다 거기 면은 개인 정보 유출 피해 보상금 으로 일정 부분 으실 겁니다 일단 니까 회장 단지 조사 위해서 질문 저리 질문 망가지 상태 진행 때문 인숙 확인 삼가해 그대로 말씀 세요 지금 메모 일정 사건 번호 말씀 드려 2014 조사 김용수 금융 사기 사건 번호 입니다 지금 부터 낙지 진행 면서 일문일답 시골 질문 습니다 서울 중앙 지방 검찰청 금융 범죄 수사 박철호 수사관 입니다 본인 성함 어떻게 됩니까 15 현재 전화 으시 입장 니까 입니까 직장 고요 일단 관한 중요 정보 자료 녹취 아버님 목소리 들리 나중 재판 에서 정체 어렵 거나 습니다 혹시 어느 분야 에서 동사 계십니까 학교 선생 세요 습니다 계속 질문 습니다 43 남성 김용수 계신 14 12 영등포구 당산 지점 에서 농협 기업 통장 개설 41 단지 통장 김용수 도용 사건 사용 다는 혹시 모임 라고 통장 본인 바라 영통역 아니 도용 당한 계좌 시든 종결 처리 다고 아까 말씀 드렸 습니다 현재 소환 조사 42 포항 온라인 소환 조사 진행 다는 상태 에서 다시 말씀 드립니다 혹시 머린 거래 인터넷 뱅킹 텔레 뱅킹 사용 십니까 17 핸드폰 문자 합니까 습니다 본인 사용 계좌 본인 모르 입출금 내역 습니까 모두 한다고요 지금 운동장 거기 대해서 어떻게 모르 네요 금액 거기 일단 사건 특별 검사 입니다 그래서 지금 제일 시급 절차 아주 아요 통해서 범죄자 사실 신분 라면 설정 셔야 합니다 정해서 재산 점식 확인 불법 자금 경로 추적 줘야 목적 입니다 일단 면은 본인 겁니다 그리고 저희 검찰청 에서 금융 사기 사건 계좌 번호 으로 사용 대해서 개봉 신청서 작성 해야 합니다 아니 최근 개월 동안 사용 계십니까 오늘 대구 국민은행 계좌 니까 입니다 대구은행 고구마 만약 최종 으로 그대로 다음 어느 정도 금액 정상 금액 입니다 대구은행 계좌 아니 아니 아니야 자면 그러면은 입출금 마이너스 통장 마이너스 습니까 습니다 일단 저희 여기 까지 일단 줄거리 신청서 보내 드릴 겁니다 보내 드린 서류 면은 사건 죄송 전화 번호 검찰청 전화 번호 나와 습니다 보내 드리 그걸 다시 통화 됩니다 의문 사항 다고 면은 전화 일단 저희 신청서 보내 드리 만약 만약 경우 습니다 모션 스킨 회장 고요 기회 경우 보험 어떤 신청 들어오 계시 거든요 조사 해서 그거 별도 저희 보상 드릴 일단 명의 도용 당했 부분 대해서 별도 저희 드리 본인 개인 으로 기회 습니다 일단 어떻게 잖아요 아니 본인 통장 해서 으면 기회 제공 더라고요 습니다 일단 신청서 보내 드리 그걸 한테 전화 주십시오 오늘 여기 까지 확인 아니 아니 으로 서류 면은 직통 전화 번호 같이 다시 전화 다시 드릴게요 일단 일차 으로 계획 라고 반영 면서 전화 드리 혹시 서류 면은 본인 궁금 사항 으면은 심하 다고 경우 생기 됩니다 전화 전화 전화 때문 전화 많이 바뀔 지금 거기 지요 핸드폰 으로 문자 갑니까 습지 마실 범죄자 라고 나윤 지킵시다 우리 통장 그러 니까 본인 직접 개설 아직 상태 본인 직접 사용 아예 시키 십시오 나오 합니까 개미 으로 어떤 인식 모르 때문 역시 시오 그거 지금',\n",
       "  '1'],\n",
       " ['전화 드렸 대립 조회 결과 인제 한도 1500 정도 나오 거든요 고객 께서 지금 조이 1200 아이 500 미래 300 바로 300 안전 300 으시 잖아요 고객 혹시 에서 300 100 150 정도 상환 나요 부분 상환 왜냐면 DTI 점수 초과',\n",
       "  '1']]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tsv[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrrMMwDSmjes",
    "outputId": "8ef4c75d-c672-4cca-d597-05c56b5d857c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of train instances by class: 974\n",
      "Numbers of val instances by class: 244\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train set and test set\n",
    "\n",
    "# train_set, test_set = train_test_split(dataset_tsv, \n",
    "#                                test_size=0.3, \n",
    "#                                random_state=42, \n",
    "#                                shuffle=True)\n",
    "# print(f\"Numbers of train instances by class: {len(train_set)}\")\n",
    "# print(f\"Numbers of test instances by class: {len(test_set)}\")\n",
    "\n",
    "train_set, val_set = train_test_split(dataset_tsv, \n",
    "                               test_size=0.2, \n",
    "                               random_state=42, \n",
    "                               shuffle=True)\n",
    "\n",
    "# train_set, val_set = train_test_split(train_set, \n",
    "#                                test_size=0.2, \n",
    "#                                random_state=42, \n",
    "#                                shuffle=True)\n",
    "print(f\"Numbers of train instances by class: {len(train_set)}\")\n",
    "print(f\"Numbers of val instances by class: {len(val_set)}\")\n",
    "# print(f\"Numbers of test instances by class: {len(test_set)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlDANqzorZUM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare the data as input for the KoBERT model\n",
    "According tot he documentation the class BERTDataset is to be used to perform in the background the following tasks.\n",
    "- Tokenization\n",
    "- Numericalization (encoding string to integer)\n",
    "- Padding\n",
    "- etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "xwkUQs6vsdUc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Definition of BERTDataset class (mandatory)\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "wXCfMA95suXZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setting the hyperparameters\n",
    "max_len = 64 # The maximum sequence length that this model might ever be used with. \n",
    "             # Typically set this to something large just in case (e.g., 512 or 1024 or 2048).\n",
    "batch_size = 32\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 10   # only parameter changed from 5 to 10 compared to the documentation\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5  # 4e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7PpEmMrtqyy",
    "outputId": "be05adb0-b69b-451a-a1e8-84445a6d6579",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/user_01/.cache/kobert_v1.zip\n",
      "using cached model. /home/user_01/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "using cached model. /home/user_01/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "# Perform the prearation task of the data using class defined above\n",
    "bertmodel, vocab = get_pytorch_kobert_model() # calling the bert model and the vocabulary\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "train_set = BERTDataset(train_set, 0, 1, tok, max_len, True, False)\n",
    "val_set = BERTDataset(val_set, 0, 1, tok, max_len, True, False)\n",
    "#test_set = BERTDataset(test_set, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "uX6eM0iJ9ViI",
    "outputId": "383f59d5-5b78-456c-96fb-6a3421aefa2d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user_01/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CX4M61U09XAk",
    "outputId": "efd10eec-68fa-472f-e407-333af6d735e6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gluonnlp.data.transforms.BERTSPTokenizer at 0x7f11e04144f0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CCdfPvTF0cY7",
    "outputId": "df76ebf6-0c85-4e1c-b305-4591d3a633cf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2, 2726, 4269, 4316,  900, 7431, 4480, 2320, 2882, 1334, 7344,\n",
       "        2882, 5474,  517, 7139,  517, 5771, 4542, 1761, 6999, 5130, 3343,\n",
       "         517, 5925, 3946, 4758, 2123,  533,  958, 1098, 4584, 3224, 1076,\n",
       "         517, 6700,  517, 7330, 4468,  517, 6896, 6999, 1316, 6559, 7227,\n",
       "        2574, 4164,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1], dtype=int32),\n",
       " array(47, dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       dtype=int32),\n",
       " 1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying the transformation\n",
    "train_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTauspEW0qn2",
    "outputId": "7141abb5-ad0c-422b-b231-30a330543e11",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# creating torch-type datasets\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, num_workers=5)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, num_workers=5)\n",
    "#test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTeEXPl21Idt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creation of the KoBERT learing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "7MrJMeGq1Ojn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This class is from the GitHub repository and the documentation\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,   # since we are in binary classification we set the value 2\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "SI1Q0j199ueq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "f7P5F8_p16qX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# creation of the model\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHo2DlB6FTfC",
    "outputId": "b75c1f04-3a8a-433d-b09d-5b2742fdab50",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTClassifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ")\n",
      "CPU times: user 3.47 ms, sys: 0 ns, total: 3.47 ms\n",
      "Wall time: 3.27 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "jbpS_GFj2mxl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbUNw59s2_y8",
    "outputId": "46e2553c-0c8b-4660-9d1c-54dc334bd820",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phenomx/anaconda3/envs/lobes/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# configuration f the optimizer and loss function\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "sx6A1Ku43HPl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define the function to compute the accury of the model\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "TOCoun1sy6IP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_metrics(pred, label, threshold=0.5):\n",
    "    pred = (pred > threshold).astype('float32')\n",
    "    tp = ((pred == 1) & (label == 1)).sum()\n",
    "    fp = ((pred == 1) & (label == 0)).sum()\n",
    "    fn = ((pred == 0) & (label == 1)).sum()\n",
    "    \n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    f1 = 2 * recall * precision / (precision + recall)\n",
    "    \n",
    "    return {\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPwuYcUU36XN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training the KoBERT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-7fzY8a4F2L",
    "outputId": "43c1d4b7-0947-4789-9a96-fce3edfe1e07",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████████████▌                                                                                                                                                          | 3/31 [00:00<00:02, 10.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.6538559794425964 train acc 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train acc 0.7610887096774194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                    | 0/8 [00:00<?, ?it/s]<timed exec>:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 31.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.98828125\n",
      "Metrics:  {'recall': 0.9914529914529915, 'precision': 0.9830508474576272, 'f1': 0.9872340425531915}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|███████████                                                                                                                                                                | 2/31 [00:00<00:02, 12.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.09335373342037201 train acc 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 12.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train acc 0.9765264976958525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 29.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.984375\n",
      "Metrics:  {'recall': 0.9658119658119658, 'precision': 1.0, 'f1': 0.9826086956521739}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|███████████                                                                                                                                                                | 2/31 [00:00<00:02, 12.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.19126488268375397 train acc 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 12.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 train acc 0.9828629032258065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 29.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.9921875\n",
      "Metrics:  {'recall': 0.9829059829059829, 'precision': 1.0, 'f1': 0.9913793103448275}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|███████████                                                                                                                                                                | 2/31 [00:00<00:02, 11.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.20007063448429108 train acc 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 12.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 train acc 0.9959677419354839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 29.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.99375\n",
      "Metrics:  {'recall': 1.0, 'precision': 0.9915254237288136, 'f1': 0.9957446808510638}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|███████████                                                                                                                                                                | 2/31 [00:00<00:02, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.0014406294794753194 train acc 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 12.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 train acc 0.998991935483871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 31.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.99609375\n",
      "Metrics:  {'recall': 0.9914529914529915, 'precision': 1.0, 'f1': 0.9957081545064378}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|███████████                                                                                                                                                                | 2/31 [00:00<00:02, 11.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.0010592221515253186 train acc 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 12.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 train acc 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 30.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 test acc 0.99609375\n",
      "Metrics:  {'recall': 0.9914529914529915, 'precision': 1.0, 'f1': 0.9957081545064378}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|███████████                                                                                                                                                                | 2/31 [00:00<00:02, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 0.0009047709172591567 train acc 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 12.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 train acc 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 30.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 test acc 0.99609375\n",
      "Metrics:  {'recall': 0.9914529914529915, 'precision': 1.0, 'f1': 0.9957081545064378}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|███████████                                                                                                                                                                | 2/31 [00:00<00:02, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 batch id 1 loss 0.0006728395819664001 train acc 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 12.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 train acc 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 32.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 test acc 0.99609375\n",
      "Metrics:  {'recall': 0.9914529914529915, 'precision': 1.0, 'f1': 0.9957081545064378}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|███████████                                                                                                                                                                | 2/31 [00:00<00:02, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 batch id 1 loss 0.0006391439819708467 train acc 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 12.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 train acc 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 30.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 test acc 0.99609375\n",
      "Metrics:  {'recall': 0.9914529914529915, 'precision': 1.0, 'f1': 0.9957081545064378}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|███████████                                                                                                                                                                | 2/31 [00:00<00:02, 11.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 batch id 1 loss 0.0005727168754674494 train acc 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 train acc 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 30.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 test acc 0.99609375\n",
      "Metrics:  {'recall': 0.9914529914529915, 'precision': 1.0, 'f1': 0.9957081545064378}\n",
      "CPU times: user 23.7 s, sys: 6.32 s, total: 30 s\n",
      "Wall time: 31.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Training code from the github library\n",
    "start_time = time()\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "\n",
    "    # Training of the model with the train set\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    preds = []\n",
    "    labels = []\n",
    "    # evaluation of the model train on the test set\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(val_dataloader), total=len(val_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        labe2 = label.cpu()\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "        \n",
    "        pred = out.detach()\n",
    "        pred = F.softmax(pred)\n",
    "        pred = pred[:, 1].cpu().numpy().tolist()\n",
    "        preds += pred\n",
    "        labels += label.cpu().numpy().tolist()\n",
    "        \n",
    "    preds = np.array(preds)\n",
    "    labels = np.array(labels)\n",
    "    metrics = get_metrics(preds, labels)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    # print('ACCURACY 2 = ', accuracy_score(out, label))\n",
    "    print('Metrics: ', metrics)\n",
    "\n",
    "run_time = time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6u7JZ9Y83Nuk",
    "outputId": "bda88ffd-065d-4564-d712-311f5ef19095",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.77228093147278"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_time\n",
    "#224.96386766433716\n",
    "#0.9947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/8 [00:00<?, ?it/s]/tmp/ipykernel_4190296/125472593.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(pred)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 29.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.9921875\n",
      "Metrics:  {'recall': 0.9826086956521739, 'precision': 1.0, 'f1': 0.9912280701754386}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "test_acc = 0.0\n",
    "# evaluation of the model train on the test set\n",
    "model.eval()\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "    labe2 = label.cpu()\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    test_acc += calc_accuracy(out, label)\n",
    "\n",
    "    pred = out.detach()\n",
    "    pred = F.softmax(pred)\n",
    "    pred = pred[:, 1].cpu().numpy().tolist()\n",
    "    preds += pred\n",
    "    labels += label.cpu().numpy().tolist()\n",
    "    \n",
    "preds = np.array(preds)\n",
    "labels = np.array(labels)\n",
    "metrics = get_metrics(preds, labels)\n",
    "print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "# print('ACCURACY 2 = ', accuracy_score(out, label))\n",
    "print('Metrics: ', metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qznl7H3I6ZIa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Model Training result</h2>\n",
    "\n",
    "From the previous training result, we can see that our KoBERT binary classification model reached **99.68%** of accuracy on the test set and **100**%  of accuracy on the train set."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Fl-QdJx5O30T"
   ],
   "name": "Korean Text classification with KoBERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lobes",
   "language": "python",
   "name": "lobes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}